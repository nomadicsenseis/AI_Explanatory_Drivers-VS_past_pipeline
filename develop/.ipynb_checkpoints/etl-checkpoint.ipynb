{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read emr cluster(j-3MNOUDE8S2V30) details\n",
      "Initiating EMR connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1677493527171_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-197-3-192.IBR.Local:20888/proxy/application_1677493527171_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-197-3-227.IBR.Local:8042/node/containerlogs/container_1677493527171_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "{\"namespace\": \"sagemaker-analytics\", \"cluster_id\": \"j-3MNOUDE8S2V30\", \"error_message\": null, \"success\": true, \"service\": \"emr\", \"operation\": \"connect\"}\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --cluster-id j-3MNOUDE8S2V30 --auth-type None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1677493527171_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-197-3-192.IBR.Local:20888/proxy/application_1677493527171_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-197-3-232.IBR.Local:8042/node/containerlogs/container_1677493527171_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv', 'spark.sql.legacy.parquet.datetimeRebaseModeInRead': 'LEGACY', 'spark.driver.memory': '10000M'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1677493527171_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-197-3-192.IBR.Local:20888/proxy/application_1677493527171_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-197-3-232.IBR.Local:8042/node/containerlogs/container_1677493527171_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{ \"conf\":\n",
    "    {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInRead\": \"LEGACY\",\n",
    "        \"spark.driver.memory\": \"10000M\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pyspark functions\n",
    "from pyspark.sql.functions import col, udf, isnan, when, count, lit, mean, min, max, sum, round, format_number, abs, current_date, dayofweek, size, dayofyear, \\\n",
    "                                    datediff, to_date, year, to_utc_timestamp, unix_timestamp, coalesce, months_between, substring, rank, countDistinct, collect_set, \\\n",
    "                                    first, quarter, dense_rank, desc, month, row_number, asc\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from typing import Dict\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_last_s3_partition(s3_dir_customer: str, grep_flag: bool=False, grep: str='') -> str:\n",
    "    \n",
    "    \"\"\" This functions assumes that the last partition is the last one that appears on `aws s3 ls` command\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s3_dir_customer: s3 path with churn data ending with '/'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    complete path to the last partition\n",
    "    \"\"\"\n",
    "    \n",
    "    var = os.popen(f\"aws s3 ls {s3_dir_customer} |tail -n 1 | awk '{{print $2}}'\").read()\n",
    "    os.path.join(s3_dir_customer,var[:-1])\n",
    "\n",
    "    return os.path.join(s3_dir_customer,var[:-1])\n",
    "\n",
    "def rename_multiple_columns(df: DataFrame, rename_col: Dict[str, str]) -> DataFrame:\n",
    "\n",
    "    \"\"\"This function change names in columns of a pyspark dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe with the columns we want to change.\n",
    "    rename_col: Dictionary where its keys are the old names of the columns\n",
    "    and the values are the new names of the columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with the new names updated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for old_name, new_name in rename_col.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_null_field(column: str, null_format: object) -> object:\n",
    "    \n",
    "    \"\"\"This function change a value by null.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    column: column with value to change.\n",
    "    null_format: value to change inside column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Column with null values instead of null_format.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return when(column.startswith(null_format), lit(None)).otherwise(column)\n",
    "\n",
    "def eliminate_time(date_time: str, sep: str) -> str:\n",
    "    \n",
    "    \"\"\"This function split a datetime string to eliminate time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date_time: datetime string\n",
    "    sep: separator between date and time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    String with date\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if not date_time is None:\n",
    "        return date_time.strip().split(sep)[0]\n",
    "    else:\n",
    "        return date_time\n",
    "    \n",
    "def substitute_values_column(df: DataFrame, column: str, dict_values_condition: dict) -> DataFrame:\n",
    "    \n",
    "    \"\"\"This function change the values inside a column based on a condition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: original dataframe\n",
    "    column: column to change values\n",
    "    dict_values_condition: list of dictionaries with dictionaries\n",
    "    with this structure --> {'value_1': x, 'value_2': y, 'col_condition': z}\n",
    "    where value_1 is the value to change, value_2 the substitute and\n",
    "    col_condition the column to meet the condition\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with column values changed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for dict_value in dict_values_condition:\n",
    "        if dict_value['value_1'] == 'null':\n",
    "            df = df.withColumn(column, when(col(dict_value['col_condition']).isNull(),\n",
    "                                                     dict_value['value_2']).otherwise(col(column)))\n",
    "        else:\n",
    "            df = df.withColumn(column, when(col(dict_value['col_condition']) == dict_value['value_1'],\n",
    "                                                         dict_value['value_2']).otherwise(col(column)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def coalesce_two_columns(df: DataFrame, dict_columns: dict) -> DataFrame:\n",
    "    \n",
    "    \"\"\"This function coalesce two columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: original dataframe\n",
    "    dict_columns: {col1: col2}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe two columns coalesced\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for column in dict_columns.keys():\n",
    "        df = df.withColumn(column, coalesce(col(column), dict_columns[column]))\n",
    "        \n",
    "    return df\n",
    "\n",
    "def season_extraction(date: object) -> str:\n",
    "\n",
    "    \"\"\"This function extracts season from a date value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date: datetime\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    String with season\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # \"day of year\" ranges for the northern hemisphere\n",
    "    spring = range(80, 172)\n",
    "    summer = range(172, 264)\n",
    "    fall = range(264, 355)\n",
    "    # winter = everything else\n",
    "    if date in spring:\n",
    "        season = 'spring'\n",
    "    elif date in summer:\n",
    "        season = 'summer'\n",
    "    elif date in fall:\n",
    "        season = 'fall'\n",
    "    else:\n",
    "        season = 'winter'\n",
    "\n",
    "    return season\n",
    "\n",
    "def read_s3_data_churn(s3_dir_customer: str, columns: str) -> DataFrame:\n",
    "    \n",
    "    \"\"\"This function read churn data from S3, filter and select columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s3_dir_customer: s3 path with churn data\n",
    "    columns: list of columns to filter out\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with all churn information not processed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Read and rename desired columns\n",
    "    df = spark.read.csv(s3_dir_customer, header = 'True')\n",
    "\n",
    "    # Select desired columns\n",
    "    df = df.select(columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def translate_class_code(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Translate class codes in each main class (economy, premium, business)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with sold_class_code translated to main class\n",
    "    \n",
    "    \"\"\"\n",
    "      \n",
    "    economy = [\"A\", \"B\", \"F\", \"G\", \"H\", \"K\", \"L\", \"M\", \"N\", \"O\", \"Q\", \"S\", \"V\", \"Y\", \"Z\", \"X\"]\n",
    "    premium = [\"E\", \"P\", \"T\", \"W\"]\n",
    "    business = [\"C\", \"D\", \"I\", \"J\", \"R\", \"U\"]\n",
    "    \n",
    "    df = df.withColumn('sold_class_code', when(col('sold_class_code').isin(economy),\n",
    "                                               'economy').otherwise(col('sold_class_code')))\n",
    "    df = df.withColumn('sold_class_code', when(col('sold_class_code').isin(premium),\n",
    "                                               'premium').otherwise(col('sold_class_code')))\n",
    "    df = df.withColumn('sold_class_code', when(col('sold_class_code').isin(business),\n",
    "                                               'business').otherwise(col('sold_class_code')))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cast_columns(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Cast all pyspark columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with int, double and date columns casted\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Cast gross_revenue and total_payment to double\n",
    "    df = df.withColumn(\"gross_revenue_eur\",df.gross_revenue_eur.cast('double'))\n",
    "    df = df.withColumn(\"eur_bags\",df.eur_bags.cast('double'))\n",
    "    df = df.withColumn(\"eur_seats\",df.eur_seats.cast('double'))\n",
    "    df = df.withColumn(\"eur_upgs\",df.eur_upgs.cast('double'))\n",
    "    df = df.withColumn(\"eur_others\",df.eur_others.cast('double'))\n",
    "    df = df.withColumn(\"revenue_avios\",df.revenue_avios.cast('double'))\n",
    "    df = df.withColumn(\"num_bags\",df.num_bags.cast('int'))\n",
    "    df = df.withColumn(\"num_seats\",df.num_seats.cast('int'))\n",
    "    df = df.withColumn(\"num_upgs\",df.num_upgs.cast('int'))\n",
    "    df = df.withColumn(\"num_others\",df.num_others.cast('int'))\n",
    "    # Cast disruptions\n",
    "    df = df.withColumn(\"flag_misconnection_misc\",when(col('flag_misconnection_misc')==1,lit(1)).otherwise(lit(0)).cast('int'))\n",
    "    df = df.withColumn(\"flag_dng\",when(col('flag_dng')==1,lit(1)).otherwise(lit(0)).cast('int'))\n",
    "    df = df.withColumn(\"cancelled\",when(col('cancelled')==1,lit(1)).otherwise(lit(0)).cast('int'))\n",
    "    df = df.withColumn(\"delayed_minutes_arrival\",df.delayed_minutes_arrival.cast('int'))\n",
    "    # Cast marketing permission and is_corporate\n",
    "    df = df.withColumn(\"mkt_permission\",df.mkt_permission.cast('int'))\n",
    "    df = df.withColumn(\"is_corporate\",df.is_corporate.cast('int'))\n",
    "    # Cast ticket_date to date\n",
    "    eliminateTimeUDF = udf(lambda x: eliminate_time(x, ' '))\n",
    "    df = df.withColumn('ticket_sale_date_cast', to_date(col(\"ticket_sale_date\"),\"yyyy-MM-dd\").alias(\"date_2\"))\n",
    "    df = df.withColumn('date_creation_pnr_resiber', to_date(eliminateTimeUDF(col(\"date_creation_pnr_resiber\")),\n",
    "                                                            \"yyyy-MM-dd\").alias(\"date_1\"))\n",
    "    df = df.withColumn('date', to_date(col(\"birth_date\"), \"yyyy-MM-dd\").alias(\"date\"))\n",
    "    df = df.withColumn('loc_dep_date', to_date(col(\"loc_dep_date\"),\"yyyy-MM-dd\"))\n",
    "    df = df.withColumn('loc_arr_date', to_date(col(\"loc_arr_date\"),\"yyyy-MM-dd\"))\n",
    "    df = df.withColumn('date_creation_idgoldenrecord', to_date(col(\"date_creation_idgoldenrecord\"),\"yyyy-MM-dd\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def email_agencies_process(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"This function eliminates rows with email operative containing\n",
    "    words from the black list that tries to eliminate agencies from the\n",
    "    final client scope\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe filtering some agencies\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    black_list_agencies = 'viaje|trip|flight|viaxe|halcon|emisiones|reserva|booking|billete|' + \\\n",
    "                       'travel|tour|venta|agencia|crew|hotel|viaggi|junta|xunta|dreamgo|' + \\\n",
    "                        'enjoyebre|airtip|melisur|ltnspain|unififi|triporate|book|' + \\\n",
    "                        'fly|viaja|pentamundos|vacaciones|ndc-communication|mundoterra|embarcate|' + \\\n",
    "                        'agency|grupogea|vectalia|partocrs|@airbus|@aer|millenniummenorca|@reattiva|' + \\\n",
    "                        'compras|pasajes|coniltur|playasenator|melillaexpress|' + \\\n",
    "                        'abramar|rascadomarin|revistaestretegas|@esky|confirmation|@calrom|confirmacion|' + \\\n",
    "                        'vacaciones|turmar|kanvoy|vuelo|undefined|iberiaexpress|gestion|@aer|@ulysse|@turatlantica|geographica|geostar|' + \\\n",
    "                        'terminal9|murimar|hceivissa|gaselec|@anjoca|grupogea|betweencongresos|protecmedia|recepcion|' + \\\n",
    "                        'grupolesaca|vacanze|voyage|pisamundavecindario|administracion|overture|' + \\\n",
    "                        'tierrasdelmundo|planning|properties|vuela|crucero|@dondetellevo|@iberostar|' + \\\n",
    "                        'nuevasrutas|persiguiendoelviento|viatges|ticket|airdepartment|e-savia|@reedmackay|' + \\\n",
    "                        '@globalia|.edreams.com|@trailfinders'\n",
    "    df = df.withColumn('perc_email_agency', when(col('email_operative').rlike(black_list_agencies), lit(1)).otherwise(lit(0)))\n",
    "    \n",
    "    w1 = Window.partitionBy('email_operative')\n",
    "    df = df.withColumn(\"n_ticket_email\", count(\"*\").over(w1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_features_non_filtered_data(df: DataFrame, year_1: str, year_2: str, year_3: str) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Extract window features (exchanges, not_travelled and refunded) before any filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "    year_1: flag for year - 1 from training date\n",
    "    year_2: flag for year - 2 from training date\n",
    "    year_3: flag for year - 2 from training date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with coupon_usage_code as flag feature with 3 new columns\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # id golden features level with all information (Exchanges, Not Travelled)\n",
    "    w1 = Window.partitionBy('cid')\n",
    "    df = df.withColumn('n_exchanges', sum(when((col('coupon_usage_code') == 'E') & \\\n",
    "                                               col('year').isin([int(year_3), int(year_2),int(year_1)]),\n",
    "                                               lit(1)).otherwise(lit(0))).over(w1))\n",
    "    df = df.withColumn('n_not_travelled', sum(when((col('coupon_usage_code') == 'N') & \\\n",
    "                                                   col('year').isin([int(year_3), int(year_2), int(year_1)]),\n",
    "                                                   lit(1)).otherwise(lit(0))).over(w1))\n",
    "    df = df.withColumn('n_refunded', sum(when((col('coupon_usage_code') == 'R') & \\\n",
    "                                              col('year').isin([int(year_3), int(year_2), int(year_1)]),\n",
    "                                              lit(1)).otherwise(lit(0))).over(w1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_general_features(df: DataFrame, year_1: str, year_2: str, year_3: str) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Extract features window features after filtering out T and N coupon usage code\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "    year_1: flag for year - 1 from training date\n",
    "    year_2: flag for year - 2 from training date\n",
    "    year_3: flag for year - 2 from training date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with sold_class_code translated, flight spent deviation and position\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Class code\n",
    "    df = translate_class_code(df)\n",
    "    # Mean, max and position of flight spent in with respect other passengers\n",
    "    w1 = Window.partitionBy('op_carrier_code', 'op_flight_num', 'loc_dep_date', 'sold_class_code')\n",
    "    df = df.withColumn(\"gross_revenue_valid\", when((col('op_carrier_code').isNotNull()) & \\\n",
    "                                                   (col('year').isin([int(year_3), int(year_2), int(year_1)])),\n",
    "                                                   col('gross_revenue_eur')).otherwise(np.nan))\n",
    "    df = df.withColumn(\"mean_flight_spent\", mean(\"gross_revenue_valid\").over(w1))\n",
    "    df = df.withColumn('deviation_flight_spent', (col('gross_revenue_valid')-col('mean_flight_spent'))/col('mean_flight_spent'))\n",
    "    df = df.withColumn(\"rank\", row_number().over(w1.orderBy(desc(\"gross_revenue_valid\"))))\n",
    "    df = df.withColumn(\"total_passenger_class\", count('rank').over(w1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_general_features_after(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    # Op code IB %\n",
    "    w1 = Window.partitionBy('cid')\n",
    "    df = df.withColumn(\"IB_count\", sum((when((col('op_carrier_code') == 'IB') & (col('coupon_usage_code') == 'T'),\n",
    "                                                                                lit(1)).otherwise(lit(0)))).over(w1))\n",
    "    df = df.withColumn(\"total_op_count\", sum((when((col('coupon_usage_code') == 'T'),\n",
    "                                                   lit(1)).otherwise(lit(0)))).over(w1))\n",
    "    df = df.withColumn(\"IB_pctg\", col('IB_count')/(col('total_op_count')))\n",
    "    \n",
    "    # Extract if passenger is resident\n",
    "    df = df.withColumn(\"is_resident\", when(col('pax_type_seg').contains('RESIDENT'), lit(1)).otherwise(lit(0)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_rank_features(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Extract order of purchase and flight by id_golden_record\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with ranking of flights and purchases\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Purchase and flights ranking\n",
    "    w1 = Window.partitionBy('cid')\n",
    "    df =  df.withColumn(\"rank_purchase\", dense_rank().over(w1.orderBy(desc(\"date_creation_pnr_resiber\"), 'pnr_resiber')))\n",
    "    df =  df.withColumn(\"rank_flight\", dense_rank().over(w1.orderBy(desc(\"loc_arr_date\"))))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_initial_dataframe(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Filter out first requeriments for churn analysis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_sel: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe without crew data, cast, coalesce of pnr date and pnrs\n",
    "    and change 0001-01-01 by null values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Change 0001-01-01 date by null values\n",
    "    df = df.withColumn('date_creation_pnr_resiber', create_null_field(col('date_creation_pnr_resiber'), '0001-01-01'))\n",
    "\n",
    "    # Filter rows with CREW customers\n",
    "    df = df.where(~col(\"last_name_1\").contains(\"CREWANE\"))\n",
    "\n",
    "    # Cast columns\n",
    "    df = cast_columns(df)\n",
    "\n",
    "    # Colaesce columns for null values\n",
    "    dict_columns = {'date_creation_pnr_resiber': 'ticket_sale_date_cast',\n",
    "                    'pnr_resiber': 'pnr_amadeus'}\n",
    "    df = coalesce_two_columns(df, dict_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_present_future_flags(df: DataFrame, year_1: str, year_2: str, year_3: str, date_limit: object,\n",
    "                                date_3year: object, date_2year: object, date_1year: object, date_future: object=None) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Create columns with present and future flags\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered: pyspark DataFrame\n",
    "    year_1: flag with value -1\n",
    "    year_2: flag with value -2\n",
    "    year_3: flag with value -3\n",
    "    date_limit: limit for labeling future transactions\n",
    "    date_3year: year-3 date\n",
    "    date_2year: year-2 date\n",
    "    date_1year: year-1 date\n",
    "    date_future: limit prediction date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with column year indicating periods (0 for future)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select years for training and testing\n",
    "    df = df.withColumn('year', year('date_creation_pnr_resiber'))\n",
    "    if date_future is not None:\n",
    "        df = df.where((col('date_creation_pnr_resiber') > date_limit) & \\\n",
    "                      (col('date_creation_pnr_resiber') <= date_future))\n",
    "    else:\n",
    "        df = df.where((col('date_creation_pnr_resiber') > date_limit) & \\\n",
    "                      (col('date_creation_pnr_resiber') <= date_1year))\n",
    "    df = df.withColumn('year', when((col('date_creation_pnr_resiber') > date_limit) & \\\n",
    "                                    (col('date_creation_pnr_resiber') <= date_3year), year_3).\\\n",
    "                       when((col('date_creation_pnr_resiber') > date_3year) & \\\n",
    "                            (col('date_creation_pnr_resiber') <= date_2year), year_2).\\\n",
    "                       when((col('date_creation_pnr_resiber') > date_2year) & \\\n",
    "                            (col('date_creation_pnr_resiber') <= date_1year), year_1).otherwise(lit(0)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def group_by_od(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Groupby train dataset by od with logic selection\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_train: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe grouped by od\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.groupby('cid', 'pnr_resiber', 'date_creation_pnr_resiber', 'itinerary_od', 'year').agg(\n",
    "        sum(col('gross_revenue_eur')).alias('gross_revenue_eur'),\n",
    "        first(col('prime_ticket_num')).alias('prime_ticket_num'),\n",
    "        sum(col('num_bags')).alias('num_bags'),\n",
    "        sum(col('eur_bags')).alias('eur_bags'),\n",
    "        sum(col('num_seats')).alias('num_seats'),\n",
    "        sum(col('eur_seats')).alias('eur_seats'),\n",
    "        sum(col('num_upgs')).alias('num_upgs'),\n",
    "        sum(col('eur_upgs')).alias('eur_upgs'),\n",
    "        sum(col('num_others')).alias('num_others'),\n",
    "        sum(col('eur_others')).alias('eur_others'),\n",
    "        max(col('ff_num')).alias('ff_num'),\n",
    "        first(col('date')).alias('date'),\n",
    "        first(col('pax_type_ind')).alias('pax_type_ind'),\n",
    "        first(col('ind_reason_business')).alias('ind_reason_business'),\n",
    "        first(col('num_days_anticipation')).alias('num_days_anticipation'),\n",
    "        first(col('num_hours_in_destination')).alias('num_hours_in_destination'),\n",
    "        min(col('loc_dep_date')).alias('loc_dep_date'),\n",
    "        max(col('loc_arr_date')).alias('loc_arr_date'),\n",
    "        min(col('loc_dep_time')).alias('loc_dep_time'),\n",
    "        max(col('loc_arr_time')).alias('loc_arr_time'),\n",
    "        first(col('date_creation_idgoldenrecord')).alias('date_creation_idgoldenrecord'),\n",
    "        first(col('min_haul')).alias('min_haul'),\n",
    "        max(when(col('ff_group') == 'IBP', col('ff_tier')).otherwise(0)).alias('ff_tier'),\n",
    "        max(when(col('ff_group') == 'IBP', col('ff_group')).otherwise(0)).alias('ff_group'),\n",
    "        sum(col('flag_misconnection_misc')).alias('flag_misconnection_misc'),\n",
    "        sum(col('flag_dng')).alias('flag_dng'),\n",
    "        max(col('delayed_minutes_arrival')).alias('max_delayed_minutes_arrival'),\n",
    "        min(col('delayed_minutes_arrival')).alias('min_delayed_minutes_arrival'),\n",
    "        sum(col('cancelled')).alias('cancelled'),\n",
    "        max(col('mkt_permission')).alias('mkt_permission'),\n",
    "        max(col('purchases_2015')).alias('purchases_2015'),\n",
    "        max(col('purchases_2016')).alias('purchases_2016'),\n",
    "        max(col('purchases_2017')).alias('purchases_2017'),\n",
    "        first(col('n_exchanges')).alias('n_exchanges'),\n",
    "        first(col('n_not_travelled')).alias('n_not_travelled'),\n",
    "        first(col('n_refunded')).alias('n_refunded'),\n",
    "        max(col('deviation_flight_spent')).alias('deviation_flight_spent'),\n",
    "        first('ind_direct_sale').alias('ind_direct_sale'),\n",
    "        first('is_corporate').alias('is_corporate'),\n",
    "        first('destination_city_od').alias('destination_city_od'),\n",
    "        sum(col('revenue_avios')).alias('revenue_avios'),\n",
    "        max(col('coupon_usage_code')).alias('coupon_usage_code'),\n",
    "        max(col('delayed_minutes_arrival')).alias('delayed_minutes_arrival'),\n",
    "        first(col('IB_pctg')).alias('IB_pctg'),\n",
    "        max(col('is_resident')).alias('is_resident'),\n",
    "        max(col('perc_email_agency')).alias('perc_email_agency'),\n",
    "        first(col('n_ticket_email')).alias('n_ticket_email'),\n",
    "        first(col('point_of_sale')).alias('point_of_sale'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def class_code_generation(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Groupby cid an creates 3 columns indicating\n",
    "    the % of each class flown (must sum 1 all columns)\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_train: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe grouped by cid with 3 columns by class\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.groupby('pnr_resiber', 'date_creation_pnr_resiber', 'cid', 'sold_class_code', 'year').count()\n",
    "    df = df.groupby('cid').agg((sum(when((col('sold_class_code') == 'economy'),\n",
    "                                                    lit(1)).otherwise(0))/sum(lit(1))).alias('economy'),\n",
    "                                          (sum(when((col('sold_class_code') == 'premium'),\n",
    "                                                    lit(1)).otherwise(0))/sum(lit(1))).alias('premium'),\n",
    "                                          (sum(when((col('sold_class_code') == 'business'),\n",
    "                                                    lit(1)).otherwise(0))/sum(lit(1))).alias('business'))\n",
    "    df = df.withColumnRenamed('cid', 'cid2')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_od_information(df: DataFrame, date_train: str) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Process that extract features at od level after grouping it.\n",
    "    Feature examples_ seniority, age, weekend flights %, etc.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_train: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with new features columns at od level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract seniority\n",
    "    df = df.withColumn(\"seniority_1\", round(datediff(to_date(lit(date_train)),col(\"date_creation_idgoldenrecord\"))))\n",
    "    df = df.withColumn(\"seniority_2\", round(datediff(to_date(lit(date_train)),col(\"date_creation_pnr_resiber\"))))\n",
    "    df = df.withColumn(\"seniority\", when((col('seniority_1') < 0) | (col('seniority_1').isNull()), col('seniority_2')).otherwise(col('seniority_1')))\n",
    "    \n",
    "    # Extract age\n",
    "    df = df.withColumn(\"age\", round(months_between(to_date(lit(date_train)),col(\"date\"))/lit(12)))\n",
    "    df = df.withColumn(\"age\", when((col('age') >= 100) | (col('age') <= 0), lit(None)).otherwise(col('age')))\n",
    "    \n",
    "    # Extract anticipation days\n",
    "    df = df.withColumn('num_days_anticipation', datediff(col('loc_dep_date'),col('date_creation_pnr_resiber')))\n",
    "    \n",
    "    # Extract weekend or weekday departure date (0 weekday, 1 weekend)\n",
    "    df = df.withColumn('loc_dep_date_day_week', dayofweek('loc_dep_date'))\n",
    "    df = df.withColumn('loc_dep_date_day_weekend', when(col('loc_dep_date_day_week').isin([1,7,6]),\n",
    "                                                        lit(1)).otherwise(0))\n",
    "    df = df.withColumn('loc_dep_date_day_week', when(col('loc_dep_date_day_week').isin([2,3,4,5]),\n",
    "                                                     lit(1)).otherwise(0))\n",
    "    \n",
    "    # Extract season of year\n",
    "    df = df.withColumn('day_of_year', dayofyear(col('loc_dep_date')))\n",
    "    extractSeasonUDF = udf(lambda x: season_extraction(x))\n",
    "    df = df.withColumn('season', extractSeasonUDF(col(\"day_of_year\")))\n",
    "    \n",
    "    # Apply window function to obtain values for conditions\n",
    "    w1 = Window.partitionBy('pnr_resiber', 'date_creation_pnr_resiber', 'cid')\n",
    "    w2 = Window.partitionBy('pnr_resiber', 'date_creation_pnr_resiber')\n",
    "    w3 = Window.partitionBy('cid')\n",
    "    w4 = Window.partitionBy('cid', 'itinerary_od_by_pnr')\n",
    "    w5 = Window.partitionBy('cid', 'destination_city_od')\n",
    "    w6 = Window.partitionBy('cid', 'pnr_resiber', 'date_creation_pnr_resiber',\n",
    "                            'prime_ticket_num').orderBy(asc('loc_arr_time'))\n",
    "    # Min and max departure and arrival date\n",
    "    df = df.withColumn(\"max_loc_arr_date\", max(\"loc_arr_date\").over(w1))\n",
    "    df = df.withColumn(\"min_loc_dep_date\", min(\"loc_dep_date\").over(w1))\n",
    "    df = df.withColumn(\"min_loc_arr_date\", min(\"loc_arr_date\").over(w1))\n",
    "    \n",
    "    # Extract number of passenger\n",
    "    df = df.withColumn(\"n_passengers\", size(collect_set(\"cid\").over(w2)))\n",
    "    \n",
    "    # Delay flag\n",
    "    df = df.withColumn(\"flight_delay\",when((col('delayed_minutes_arrival')>=15) & (col('min_haul') == 'SH'),\n",
    "                                           lit(1)).otherwise(lit(0)))\n",
    "    df = df.withColumn(\"flight_delay\",when((col('delayed_minutes_arrival')>=20) & (col('min_haul') == 'MH'),\n",
    "                                           lit(1)).otherwise(col('flight_delay')))\n",
    "    df = df.withColumn(\"flight_delay\",when((col('delayed_minutes_arrival')>=30) & (col('min_haul') == 'LH'),\n",
    "                                           lit(1)).otherwise(col('flight_delay')))\n",
    "    \n",
    "    # Extract first od from pnr\n",
    "    df = df.withColumn(\"itinerary_od_by_pnr\", first(\"itinerary_od\").over(w1.orderBy('loc_arr_date')))\n",
    "    df = df.withColumn(\"od_distinct\", size(collect_set(\"itinerary_od_by_pnr\").over(w3)))\n",
    "    df = df.withColumn(\"n_ods\", size(collect_set(\"pnr_resiber\").over(w4)))\n",
    "    df = df.withColumn(\"n_ods_total\", size(collect_set(\"pnr_resiber\").over(w3)))\n",
    "    \n",
    "    # Extract number of distinct destination od\n",
    "    df = df.withColumn(\"n_destination_city_od\", size(collect_set(\"destination_city_od\").over(w3)))\n",
    "    \n",
    "    # Extract most common destination city\n",
    "    df = df.withColumn(\"n_order_od\", row_number().over(w6))\n",
    "    df = df.withColumn(\"n_order_od\", when(col('coupon_usage_code') == 'N', lit(None)).otherwise(col('n_order_od')))\n",
    "    df = df.withColumn(\"n_order_od_max\", max(col('n_order_od')).over(w6))\n",
    "    df = df.withColumn(\"destination_city_od_filt\", when((col('n_order_od') == col('n_order_od_max')) & (col('n_order_od') != 1),\n",
    "                                                        lit(None)).otherwise(col('destination_city_od')))\n",
    "    df = df.withColumn(\"n_dest_cid\", count(\"destination_city_od_filt\").over(w5))\n",
    "    df = df.withColumn(\"n_most_od_flown\", max(when(col('destination_city_od_filt') != 'MAD',\n",
    "                                                   col(\"n_dest_cid\")).otherwise(lit(0))).over(w3))\n",
    "    df = df.withColumn(\"most_od_flown\", max(when(col('n_most_od_flown') == col('n_dest_cid'),\n",
    "                                                 col('destination_city_od_filt')).otherwise(lit(None))).over(w3))\n",
    "    \n",
    "    w1 = Window.partitionBy('cid')\n",
    "    # Extract flag delay last flight\n",
    "    df = df.withColumn('n_flight_order_flown', (when(col('coupon_usage_code') == 'T',\n",
    "                                               row_number().over(w1.orderBy(desc('loc_dep_date')))).otherwise(lit(None))))\n",
    "    df = df.withColumn('min_n_flight_order_flown', min('n_flight_order_flown').over(w1))\n",
    "    df = df.withColumn('flag_last_flight_delay',\n",
    "                       max(when((col('n_flight_order_flown') == col('min_n_flight_order_flown')) & \\\n",
    "                                (col('min_haul') == 'SH') & \\\n",
    "                                (col('delayed_minutes_arrival') > 15), lit(1)) \\\n",
    "                                .otherwise(when((col('n_flight_order_flown') == col('min_n_flight_order_flown')) & \\\n",
    "                                                (col('min_haul').isin('MH', 'LH')) & \\\n",
    "                                                (col('delayed_minutes_arrival') > 30),\n",
    "                                                lit(1)).otherwise(lit(0)))).over(w1))\n",
    "    df = df.withColumn('n_flight_order', row_number().over(w1.orderBy(desc('loc_dep_date'))))\n",
    "    df = df.withColumn('min_n_flight_order', min('n_flight_order').over(w1))\n",
    "    df = df.withColumn('flag_last_flight_cancelled',\n",
    "                       max(when((col('n_flight_order') == col('min_n_flight_order')) & \\\n",
    "                                (col('cancelled') == 1), lit(1)) \\\n",
    "                                .otherwise(lit(0))).over(w1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_flags_weeks(df: DataFrame, date_1year: object, date_last_week: object) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Creates a flag indicating if the row comes from a last week flight\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_train: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with new flag columns indicating if it\n",
    "    comes from last week\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.withColumn('last_week', when((col('loc_dep_date') >= date_last_week) & \\\n",
    "                                         (col('loc_dep_date') <= date_1year), lit(1)).otherwise(0))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_features_cid_level(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Groupby a dataframe at od level by cid to extract\n",
    "    features such as flight deviation spent, mkt permission or disruptions\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_train_od: pyspark DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with features columns at od level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.groupby('cid').agg((mean(col('deviation_flight_spent'))).alias('mean_deviation_flight_spent'),\n",
    "                                          max(col('mkt_permission')).alias('mkt_permission'),\n",
    "                                          max(col('purchases_2015')).alias('purchases_2015'),\n",
    "                                          max(col('purchases_2016')).alias('purchases_2016'),\n",
    "                                          max(col('purchases_2017')).alias('purchases_2017'),\n",
    "                                          sum(when(col('last_week') == 1, col('flag_misconnection_misc')).otherwise(lit(0))).alias('flag_misconnection_misc_last_week'),\n",
    "                                          sum(when(col('last_week') == 1, col('flag_dng')).otherwise(lit(0))).alias('flag_dng_last_week'),\n",
    "                                          sum(when(col('last_week') == 1, col('cancelled')).otherwise(lit(0))).alias('flight_cnld_last_week'),\n",
    "                                          sum(when(col('last_week') == 1, col('flight_delay')).otherwise(lit(0))).alias('flight_delay_last_week'),\n",
    "                                          first('most_od_flown').alias('most_od_flown'))\n",
    "\n",
    "    df = df.withColumnRenamed('cid', 'cid2')\n",
    "\n",
    "    return df\n",
    "\n",
    "def target_dataframe(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Creates target columns for CLTV from a dataframe\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_future: pyspark DataFrame for testing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with targets columns\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.groupby('cid').agg(sum(col('gross_revenue_eur') + (col('eur_bags') + col('eur_seats') + col('eur_upgs') + col('eur_others'))).alias('gross_revenue_future'))\n",
    "    df = df.withColumnRenamed('cid', 'cid2')\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_dataframe(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Creates dataframe at pnr level, cid level\n",
    "    with all posible features and creates flags for last Qs\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_train_od: pyspark DataFrame for training\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark Dataframe with new features at pnr/cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.groupby('pnr_resiber', 'date_creation_pnr_resiber', 'year', 'cid')\\\n",
    "        .agg(sum(col('gross_revenue_eur')).alias('gross_revenue_eur'),\n",
    "         countDistinct(col('itinerary_od')).alias('n_flights'),\n",
    "         sum((col('num_bags') + col('num_seats') + col('num_upgs') + col('num_others'))).alias('n_ancillaries'),\n",
    "         sum((col('eur_bags') + col('eur_seats') + col('eur_upgs') + col('eur_others'))).alias('eur_ancillaries'),\n",
    "         sum((col('num_bags'))).alias('num_bags'), sum((col('num_seats'))).alias('num_seats'), sum((col('num_upgs'))).alias('num_upgs'),\n",
    "         sum((col('num_others'))).alias('num_others'), sum((col('eur_bags'))).alias('eur_bags'), sum((col('eur_seats'))).alias('eur_seats'),\n",
    "         sum((col('eur_upgs'))).alias('eur_upgs'), sum((col('eur_others'))).alias('eur_others'),\n",
    "         max('ff_num').alias('flag_ibplus'),\n",
    "         max('pax_type_ind').alias('pax_type_ind'),\n",
    "         max('age').alias('age'),\n",
    "         min(col('num_days_anticipation')).alias('num_days_anticipation'),\n",
    "         min(col('num_hours_in_destination')).alias('num_hours_in_destination'),\n",
    "         max(col('ind_reason_business')).alias('ind_reason_business'),\n",
    "         sum(col('loc_dep_date_day_weekend')).alias('loc_dep_date_day_weekend'),\n",
    "         (max('n_passengers')-1).alias('n_passengers'),\n",
    "         first(col('season')).alias('season'),\n",
    "         max(col('seniority')).alias('seniority'),\n",
    "         max(col('max_loc_arr_date')).alias('max_loc_arr_date'),\n",
    "         max(col('min_haul')).alias('haul'),\n",
    "         max(when(col('ff_group') == 'IBP', col('ff_tier')).otherwise(0)).alias('ff_tier'),\n",
    "         max(col('max_delayed_minutes_arrival')).alias('max_minutes_delay_arr'),\n",
    "         min(col('min_delayed_minutes_arrival')).alias('min_minutes_delay_arr'),\n",
    "         first(col('n_exchanges')).alias('n_exchanges'),\n",
    "         first(col('n_not_travelled')).alias('n_not_travelled'),\n",
    "         first(col('n_refunded')).alias('n_refunded'),\n",
    "         first(col('rank_purchase')).alias('rank_purchase'),\n",
    "         first(col('rank_flight')).alias('rank_flight'),\n",
    "         first('n_ods').alias('n_ods'),\n",
    "         first('n_ods_total').alias('n_ods_total'),\n",
    "         first('ind_direct_sale').alias('ind_direct_sale'),\n",
    "         first('is_corporate').alias('is_corporate'),\n",
    "         first('n_destination_city_od').alias('n_destination_city_od'),\n",
    "         sum(col('revenue_avios')).alias('revenue_avios'),\n",
    "         sum(col('flight_delay')).alias('flight_delay'),\n",
    "         first('IB_pctg').alias('IB_pctg'),\n",
    "         max('is_resident').alias('is_resident'),\n",
    "         max('flag_last_flight_delay').alias('flag_last_flight_delay'),\n",
    "         max('perc_email_agency').alias('perc_email_agency'),\n",
    "         first('n_ticket_email').alias('n_ticket_email'),\n",
    "         first(col('point_of_sale')).alias('point_of_sale'))\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df: DataFrame, date_train: str, date_1year: object, date_2year: object, date_3year: object,\n",
    "                    date_limit: object, date_last_week: object, date_future: object=None) -> object:\n",
    "    \n",
    "    \"\"\"Main function that preprocess original dataset and extract all posible\n",
    "    features dividing the data between training and future (label) dataset\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: original pyspark DataFrame\n",
    "    date_future: date limit for labeling data\n",
    "    date_train: string that indicates limit for training\n",
    "    date_1year: date limit for year-1\n",
    "    date_2year: date limit for year-2\n",
    "    date_3year: date limit for year-3\n",
    "    date_limit: last date for training data\n",
    "    date_last_week: date limit for recent week\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    two pyspark dataframes containing features at pnr level and label dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out requirements\n",
    "    df = filter_initial_dataframe(df)\n",
    "    # Select years for training and testing\n",
    "    year_1, year_2, year_3 = -1, -2, -3\n",
    "    df = create_present_future_flags(df, year_1, year_2, year_3, date_limit,\n",
    "                                     date_3year, date_2year, date_1year, date_future)\n",
    "    # Extract general features from non-filtered data\n",
    "    df = extract_features_non_filtered_data(df, year_1, year_2, year_3)\n",
    "    # Use Travelled and Not Travelled and eliminate frees\n",
    "    df = df.where((col('coupon_usage_code').isin(['T', 'N'])) & (col('revenue_pax_ind') == 'Y'))\n",
    "    # Extract general features from flights\n",
    "    df = extract_general_features(df, year_1, year_2, year_3)\n",
    "    # Filter rows with no cid\n",
    "    df = df.where(col(\"cid\").isNotNull())\n",
    "    # Filter agencies\n",
    "    df = email_agencies_process(df)\n",
    "    # Divide dataframe between past and future\n",
    "    df_train = df.where(col('year').isin([year_1, year_2, year_3]))\n",
    "    df_future = df.where(col('year') == 0)\n",
    "    # Replace DO by SH in Haul feature\n",
    "    df_train = df_train.withColumn('haul', when(col('haul') == 'DO', 'SH').otherwise(col('haul')))\n",
    "    # Extract general features without future\n",
    "    df_train = extract_general_features_after(df_train)\n",
    "    # Min and max departure and arrival date by od\n",
    "    w1 = Window.partitionBy('cid', 'pnr_resiber', 'date_creation_pnr_resiber', 'itinerary_od')\n",
    "    df_train = df_train.withColumn(\"min_loc_dep_date\", min(\"loc_dep_date\").over(w1))\n",
    "    df_train = df_train.withColumn(\"max_loc_arr_date\", max(\"loc_arr_date\").over(w1))\n",
    "    df_train = df_train.withColumn(\"min_haul\", min(\"haul\").over(w1))\n",
    "\n",
    "    # IBPlus Tier\n",
    "    dict_values_condition = [{'col_condition': 'ff_tier', 'value_1': 'null', 'value_2': lit(0)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Clasica', 'value_2': lit(1)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Plata', 'value_2': lit(2)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Oro', 'value_2': lit(3)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Platino', 'value_2': lit(4)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Infinita', 'value_2': lit(5)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Infinita Prime', 'value_2': lit(6)},\n",
    "                             {'col_condition': 'ff_tier', 'value_1': 'Singular', 'value_2': lit(7)}]\n",
    "    df_train = substitute_values_column(df_train, 'ff_tier', dict_values_condition)\n",
    "    \n",
    "    # Extract class code at flight level (Not OD level)\n",
    "    df_class_code = class_code_generation(df_train)\n",
    "    \n",
    "    # Group all data by od\n",
    "    df_train_od = group_by_od(df_train)\n",
    "        \n",
    "    # Extract ranking purchase and flights\n",
    "    df_train_od = extract_rank_features(df_train_od)\n",
    "    \n",
    "    # Extract n_passenger and od information\n",
    "    df_train_od = extract_od_information(df_train_od, date_train)\n",
    "    \n",
    "    # Create flag column for last week\n",
    "    df_train_od = extract_flags_weeks(df_train_od, date_1year, date_last_week)\n",
    "\n",
    "    # Extract information at cid level\n",
    "    df_cid = extract_features_cid_level(df_train_od)\n",
    "\n",
    "    # Aggregate data with customer logic (PNR + DATE + CID = 1 PURCHASE) and select important variables\n",
    "    # Target\n",
    "    df_future = target_dataframe(df_future)\n",
    "    # Train\n",
    "    df_train = train_dataframe(df_train_od)\n",
    "    # Join first level features\n",
    "    df_train = df_train.join(df_class_code,df_train[\"cid\"] == df_class_code[\"cid2\"], \"left_outer\")\n",
    "    df_train = df_train.join(df_cid,df_train[\"cid\"] == df_cid[\"cid2\"],  \"left_outer\")\n",
    "    \n",
    "    return df_train, df_future\n",
    "\n",
    "def post_features_extraction(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Function that extract 4 columns with mean spend after grouping all\n",
    "    data at cid level\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_training_features: pyspark Dataframe with all features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Final pyspark dataframe with all features\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.withColumn('mean_spend', col('gross_revenue_and_ancillaries')/col('frequency'))\n",
    "    df = df.withColumn('mean_spend_year3', col('gross_revenue_and_ancillaries_year3')/col('frequency_year3'))\n",
    "    df = df.withColumn('mean_spend_year2', col('gross_revenue_and_ancillaries_year2')/col('frequency_year2'))\n",
    "    df = df.withColumn('mean_spend_year1', col('gross_revenue_and_ancillaries_year1')/col('frequency_year1'))\n",
    "    df = df.fillna(0, subset=['mean_spend_year3', 'mean_spend_year2', 'mean_spend_year1'])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def features_frequency(year_1: str, year_2: str, year_3: str, date_train: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about frequency or purchase and flight\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    date_train: string containing limit date for training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Frequency\n",
    "    f_total = (sum(lit(1))).alias('frequency')\n",
    "    f_3 = (sum(when(col('year') == int(year_3), lit(1)).otherwise(0))).alias('frequency_year3')\n",
    "    f_2 = (sum(when(col('year') == int(year_2), lit(1)).otherwise(0))).alias('frequency_year2')\n",
    "    f_1 = (sum(when(col('year') == int(year_1), lit(1)).otherwise(0))).alias('frequency_year1')\n",
    "    \n",
    "    # Last purchase distance\n",
    "    last_purchase = (datediff(to_date(lit(date_train)), max(when((col('rank_purchase') == 1), col('date_creation_pnr_resiber')).otherwise(to_date(lit('01-01-0001')))))).alias('last_purchase')\n",
    "    last2_purchase = (datediff(to_date(lit(date_train)), max(when((col('rank_purchase') == 2), col('date_creation_pnr_resiber')).otherwise(to_date(lit('01-01-0001')))))).alias('last2_purchase')\n",
    "    last3_purchase = (datediff(to_date(lit(date_train)), max(when((col('rank_purchase') == 3), col('date_creation_pnr_resiber')).otherwise(to_date(lit('01-01-0001')))))).alias('last3_purchase')\n",
    "    # Last flight\n",
    "    last_flight = (datediff(to_date(lit(date_train)), max((col('max_loc_arr_date'))))).alias('last_flight')\n",
    "\n",
    "    purchases_2015 = max(col('purchases_2015')).alias('purchases_2015')\n",
    "    purchases_2016 = max(col('purchases_2016')).alias('purchases_2016')\n",
    "    purchases_2017 = max(col('purchases_2017')).alias('purchases_2017')\n",
    "    \n",
    "    return (f_total, f_3, f_2, f_1, last_purchase, last2_purchase, last3_purchase,\n",
    "            last_flight, purchases_2015, purchases_2016, purchases_2017)\n",
    "\n",
    "def features_gross_revenue(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about gross revenue\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Year\n",
    "    gr_total = (sum(col('gross_revenue_eur'))).alias('gross_revenue_eur')\n",
    "    gr_anc_total = (sum(col('gross_revenue_eur') + col('eur_ancillaries'))).alias('gross_revenue_and_ancillaries')\n",
    "    gr_3 = (sum(when(col('year') == int(year_3), col('gross_revenue_eur')).otherwise(0))).alias('gross_revenue_year3')\n",
    "    gr_anc_3 = (sum(when(col('year') == int(year_3), col('gross_revenue_eur') + col('eur_ancillaries')).otherwise(0))).alias('gross_revenue_and_ancillaries_year3')\n",
    "    gr_2 = (sum(when(col('year') == int(year_2), col('gross_revenue_eur')).otherwise(0))).alias('gross_revenue_year2')\n",
    "    gr_anc_2 = (sum(when(col('year') == int(year_2), col('gross_revenue_eur') + col('eur_ancillaries')).otherwise(0))).alias('gross_revenue_and_ancillaries_year2')\n",
    "    gr_1 = (sum(when(col('year') == int(year_1), col('gross_revenue_eur')).otherwise(0))).alias('gross_revenue_year1')\n",
    "    gr_anc_1 = (sum(when(col('year') == int(year_1), col('gross_revenue_eur') + col('eur_ancillaries')).otherwise(0))).alias('gross_revenue_and_ancillaries_year1')\n",
    "    \n",
    "    # Gross revenue by haul\n",
    "    gr_1_sh = (sum(when((col('year') == int(year_1)) & (col('haul') == 'SH'), col('gross_revenue_eur')).otherwise(0))).alias('gross_revenue_sh_year1')\n",
    "    gr_1_mh = (sum(when((col('year') == int(year_1)) & (col('haul') == 'MH'), col('gross_revenue_eur')).otherwise(0))).alias('gross_revenue_mh_year1')\n",
    "    gr_1_lh = (sum(when((col('year') == int(year_1)) & (col('haul') == 'LH'), col('gross_revenue_eur')).otherwise(0))).alias('gross_revenue_lh_year1')\n",
    "    \n",
    "    # Last spend\n",
    "    last_spent = (max(when((col('rank_purchase') == 1), col('gross_revenue_eur') + col('eur_ancillaries')).otherwise(0))).alias('last_spent')\n",
    "\n",
    "    return (gr_total, gr_3, gr_2, gr_1, gr_anc_total, gr_anc_3, gr_anc_2, gr_anc_1, last_spent, gr_1_sh, gr_1_mh, gr_1_lh)\n",
    "\n",
    "def features_number_ancillaries(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about number of ancillaries\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_anc_total = (sum(col('n_ancillaries'))).alias('n_ancillaries')\n",
    "    n_bags_total = (sum(col('num_bags'))).alias('num_bags')\n",
    "    n_seats_total = (sum(col('num_seats'))).alias('num_seats')\n",
    "    n_upgs_total = (sum(col('num_upgs'))).alias('num_upgs')\n",
    "    n_others_total = (sum(col('num_others'))).alias('num_others')\n",
    "    n_anc_3 = (sum(when(col('year') == int(year_3), col('n_ancillaries')).otherwise(0))).alias('n_ancillaries_year3')\n",
    "    n_anc_2 = (sum(when(col('year') == int(year_2), col('n_ancillaries')).otherwise(0))).alias('n_ancillaries_year2')\n",
    "    n_anc_1 = (sum(when(col('year') == int(year_1), col('n_ancillaries')).otherwise(0))).alias('n_ancillaries_year1')\n",
    "\n",
    "    return n_anc_total, n_anc_3, n_anc_2, n_anc_1, n_bags_total, n_seats_total, n_upgs_total, n_others_total\n",
    "\n",
    "def features_euros_ancillaries(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about ancillaries prices\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    eur_anc_total = (sum(col('eur_ancillaries'))).alias('eur_ancillaries')\n",
    "    eur_bags_total = (sum(col('eur_bags'))).alias('eur_bags')\n",
    "    eur_seats_total = (sum(col('eur_seats'))).alias('eur_seats')\n",
    "    eur_upgs_total = (sum(col('eur_upgs'))).alias('eur_upgs')\n",
    "    eur_others_total = (sum(col('eur_others'))).alias('eur_others')\n",
    "    eur_anc_3 = (sum(when(col('year') == int(year_3), col('eur_ancillaries')).otherwise(0))).alias('eur_ancillaries_year3')\n",
    "    eur_anc_2 = (sum(when(col('year') == int(year_2), col('eur_ancillaries')).otherwise(0))).alias('eur_ancillaries_year2')\n",
    "    eur_anc_1 = (sum(when(col('year') == int(year_1), col('eur_ancillaries')).otherwise(0))).alias('eur_ancillaries_year1')\n",
    "\n",
    "    return eur_anc_total, eur_anc_3, eur_anc_2, eur_anc_1, eur_bags_total, eur_seats_total, eur_upgs_total, eur_others_total\n",
    "\n",
    "def features_days_anticipation(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about days of anticipation\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_anticip_total = (sum(when((col('num_days_anticipation').isNotNull()), col('num_days_anticipation')).otherwise(0))\\\n",
    "                       /sum(when((col('num_days_anticipation').isNotNull()), lit(1)).otherwise(0))).alias('num_days_anticipation')\n",
    "    n_anticip_3 = (sum(when((col('year') == int(year_3)) & (col('num_days_anticipation').isNotNull()),\n",
    "                            col('num_days_anticipation')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_3))  & (col('num_days_anticipation').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('num_days_anticipation_year3')\n",
    "    n_anticip_2 = (sum(when((col('year') == int(year_2)) & (col('num_days_anticipation').isNotNull()),\n",
    "                            col('num_days_anticipation')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_2))  & (col('num_days_anticipation').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('num_days_anticipation_year2')\n",
    "    n_anticip_1 = (sum(when((col('year') == int(year_1)) & (col('num_days_anticipation').isNotNull()),\n",
    "                            col('num_days_anticipation')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_1))  & (col('num_days_anticipation').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('num_days_anticipation_year1')\n",
    "    \n",
    "\n",
    "    return n_anticip_total, n_anticip_3, n_anticip_2, n_anticip_1\n",
    "\n",
    "def features_hours_destination(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about hours in destination\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    hours_dest_total = (sum(when((col('num_hours_in_destination').isNotNull()), col('num_hours_in_destination')).otherwise(0))\\\n",
    "                       /sum(when((col('num_hours_in_destination').isNotNull()), lit(1)).otherwise(0))).alias('num_hours_in_destination')\n",
    "    hours_dest_3 = (sum(when((col('year') == int(year_3)) & (col('num_hours_in_destination').isNotNull()),\n",
    "                            col('num_hours_in_destination')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_3))  & (col('num_hours_in_destination').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('num_hours_in_destination_year3')\n",
    "    hours_dest_2 = (sum(when((col('year') == int(year_2)) & (col('num_hours_in_destination').isNotNull()),\n",
    "                            col('num_hours_in_destination')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_2))  & (col('num_hours_in_destination').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('num_hours_in_destination_year2')\n",
    "    hours_dest_1 = (sum(when((col('year') == int(year_1)) & (col('num_hours_in_destination').isNotNull()),\n",
    "                            col('num_hours_in_destination')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_1))  & (col('num_hours_in_destination').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('num_hours_in_destination_year1')\n",
    "    \n",
    "\n",
    "    return hours_dest_total, hours_dest_3, hours_dest_2, hours_dest_1\n",
    "\n",
    "def features_reason_business(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % business reason\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ind_reason_total = (sum(when((col('ind_reason_business').isNotNull()), col('ind_reason_business')).otherwise(0))\\\n",
    "                       /sum(when((col('ind_reason_business').isNotNull()), lit(1)).otherwise(0))).alias('ind_reason_business')\n",
    "    ind_reason_3 = (sum(when((col('year') == int(year_3)) & (col('ind_reason_business').isNotNull()),\n",
    "                            col('ind_reason_business')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_3))  & (col('ind_reason_business').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('ind_reason_business_year3')\n",
    "    ind_reason_2 = (sum(when((col('year') == int(year_2)) & (col('ind_reason_business').isNotNull()),\n",
    "                            col('ind_reason_business')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_2))  & (col('ind_reason_business').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('ind_reason_business_year2')\n",
    "    ind_reason_1 = (sum(when((col('year') == int(year_1)) & (col('ind_reason_business').isNotNull()),\n",
    "                            col('ind_reason_business')).otherwise(0))\\\n",
    "                   /sum(when((col('year') == int(year_1))  & (col('ind_reason_business').isNotNull()),\n",
    "                             lit(1)).otherwise(0))).alias('ind_reason_business_year1')\n",
    "    \n",
    "\n",
    "    return ind_reason_total, ind_reason_3, ind_reason_2, ind_reason_1\n",
    "\n",
    "def features_dep_weekend(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % departure at weekend\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dep_weekend_total = (sum(col('loc_dep_date_day_weekend'))/sum(col('n_flights'))).alias('loc_dep_weekend')\n",
    "    dep_weekend_3 = (sum(when(col('year') == int(year_3), col('loc_dep_date_day_weekend')).otherwise(0))\\\n",
    "                     /sum(when(col('year') == int(year_3), col('n_flights')).otherwise(0))).alias('loc_dep_weekend_year3')\n",
    "    dep_weekend_2 = (sum(when(col('year') == int(year_2), col('loc_dep_date_day_weekend')).otherwise(0))\\\n",
    "                     /sum(when(col('year') == int(year_2), col('n_flights')).otherwise(0))).alias('loc_dep_weekend_year2')\n",
    "    dep_weekend_1 = (sum(when(col('year') == int(year_1), col('loc_dep_date_day_weekend')).otherwise(0))\\\n",
    "                     /sum(when(col('year') == int(year_1), col('n_flights')).otherwise(0))).alias('loc_dep_weekend_year1')\n",
    "    \n",
    "    return dep_weekend_total, dep_weekend_3, dep_weekend_2, dep_weekend_1\n",
    "\n",
    "def features_ow_flights(year_1: str, year_2: str, year_3: str) -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % one way flights\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ow_flight_total = (sum(when((col('n_flights') == 1), lit(1)).otherwise(0))/sum(lit(1))).alias('ow_flights')\n",
    "    ow_flight_3 = (sum(when((col('year') == int(year_3)) & (col('n_flights') == 1), lit(1)).otherwise(0))\\\n",
    "                     /sum(when(col('year') == int(year_3), lit(1)).otherwise(0))).alias('ow_flights_year3')\n",
    "    ow_flight_2 = (sum(when((col('year') == int(year_2)) & (col('n_flights') == 1), lit(1)).otherwise(0))\\\n",
    "                     /sum(when(col('year') == int(year_2), lit(1)).otherwise(0))).alias('ow_flights_year2')\n",
    "    ow_flight_1 = (sum(when((col('year') == int(year_1)) & (col('n_flights') == 1), lit(1)).otherwise(0))\\\n",
    "                     /sum(when(col('year') == int(year_1), lit(1)).otherwise(0))).alias('ow_flights_year1')\n",
    "    \n",
    "\n",
    "    return ow_flight_total, ow_flight_3, ow_flight_2, ow_flight_1\n",
    "\n",
    "def features_ibplus() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about ibplus condition\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ibplus = (max(when((col('flag_ibplus').isNotNull()), lit(1)).otherwise(0))).alias('flag_ibplus')\n",
    "    is_corporate = (sum(col('is_corporate'))/sum(when((col('is_corporate').isNotNull()), lit(1)).otherwise(0))).alias('is_corporate')\n",
    "    revenue_avios = sum(col('revenue_avios')).alias('revenue_avios')\n",
    "    \n",
    "    return ibplus, is_corporate, revenue_avios\n",
    "\n",
    "def features_type_ind() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about passenger type\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    type_ind = max('pax_type_ind').alias('pax_type_ind')\n",
    "    is_resident = max('is_resident').alias('is_resident')\n",
    "    \n",
    "    return type_ind, is_resident\n",
    "\n",
    "def features_age() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about age\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    age = max('age').alias('age')\n",
    "    \n",
    "    return age\n",
    "\n",
    "def features_n_passengers() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % number of passenger\n",
    "    (alone, couple or groups)\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    alone_flights = (sum(when((col('n_passengers') == 0), lit(1)).otherwise(0))/sum(lit(1))).alias('alone_flights')\n",
    "    couple_flights = (sum(when((col('n_passengers') == 1), lit(1)).otherwise(0))/sum(lit(1))).alias('couple_flights')\n",
    "    group_flights = (sum(when((col('n_passengers') > 1), lit(1)).otherwise(0))/sum(lit(1))).alias('group_flights')\n",
    "    \n",
    "    return alone_flights, couple_flights, group_flights\n",
    "\n",
    "def features_season() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % flights in each\n",
    "    season\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    summer_flights = (sum(when((col('season') == 'summer'), lit(1)).otherwise(0))/sum(lit(1))).alias('summer_flights')\n",
    "    winter_flights = (sum(when((col('season') == 'winter'), lit(1)).otherwise(0))/sum(lit(1))).alias('winter_flights')\n",
    "    fall_flights = (sum(when((col('season') == 'fall'), lit(1)).otherwise(0))/sum(lit(1))).alias('fall_flights')\n",
    "    spring_flights = (sum(when((col('season') == 'spring'), lit(1)).otherwise(0))/sum(lit(1))).alias('spring_flights')\n",
    "    \n",
    "    return summer_flights, winter_flights, fall_flights, spring_flights\n",
    "\n",
    "def features_seniority() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about seniority\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seniority = max(col('seniority')).alias('seniority')\n",
    "    \n",
    "    return seniority\n",
    "\n",
    "def features_haul() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % of each haul\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sh_haul = (sum(when((col('haul') == 'SH') & (col('haul').isNotNull()), lit(1)).otherwise(0))/sum(when((col('haul').isNotNull()), lit(1)).otherwise(0))).alias('sh_haul')\n",
    "    mh_haul = (sum(when((col('haul') == 'MH') & (col('haul').isNotNull()), lit(1)).otherwise(0))/sum(when((col('haul').isNotNull()), lit(1)).otherwise(0))).alias('mh_haul')\n",
    "    lh_haul = (sum(when((col('haul') == 'LH') & (col('haul').isNotNull()), lit(1)).otherwise(0))/sum(when((col('haul').isNotNull()), lit(1)).otherwise(0))).alias('lh_haul')\n",
    "    \n",
    "    return sh_haul, mh_haul, lh_haul\n",
    "\n",
    "def features_class_code() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about % class flown\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    economy = first(col('economy')).alias('economy')\n",
    "    premium = first(col('premium')).alias('premium')\n",
    "    business = first(col('business')).alias('business')\n",
    "\n",
    "    return economy, premium, business\n",
    "\n",
    "def features_tier() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about tier condition\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ff_tier = max(col('ff_tier')).alias('ff_tier')\n",
    "    \n",
    "    return ff_tier\n",
    "\n",
    "def features_flight_spent() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about max, min and deviation\n",
    "    of flight spent/position with respect flights\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dev_spent_flight = first(col('mean_deviation_flight_spent')).alias('mean_deviation_flight_spent')\n",
    "    \n",
    "    return dev_spent_flight\n",
    "\n",
    "def features_disruptions() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about disruptions\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    flag_misconnection_misc_last_week = first(col('flag_misconnection_misc_last_week')).alias('flag_misconnection_misc_last_week')\n",
    "    flag_dng_last_week= first(col('flag_dng_last_week')).alias('flag_dng_last_week')\n",
    "    flight_cnld_last_week = first(col('flight_cnld_last_week')).alias('flight_cnld_last_week')\n",
    "    flight_delay_last_week = first(col('flight_delay_last_week')).alias('flight_delay_last_week')\n",
    "    flag_last_flight_delay = first(col('flag_last_flight_delay')).alias('flag_last_flight_delay')\n",
    "\n",
    "    return (flag_misconnection_misc_last_week, flag_dng_last_week, flight_cnld_last_week, flight_delay_last_week, flag_last_flight_delay)\n",
    "\n",
    "def features_flight_time(year_1, year_2, year_3):\n",
    "    \n",
    "    \"\"\"Function that creates conditions about delays\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_1: flag indicating year-1\n",
    "    year_2: flag indicating year-2\n",
    "    year_3: flag indicating year-3\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    max_minutes_delay_arr_1 = (max(when(col('year') == int(year_1), col('max_minutes_delay_arr')).otherwise(-np.inf))).alias('max_minutes_delay_arr_year1')\n",
    "    max_minutes_delay_arr_2 = (max(when(col('year') == int(year_2), col('max_minutes_delay_arr')).otherwise(-np.inf))).alias('max_minutes_delay_arr_year2')\n",
    "    max_minutes_delay_arr_3 = (max(when(col('year') == int(year_3), col('max_minutes_delay_arr')).otherwise(-np.inf))).alias('max_minutes_delay_arr_year3')\n",
    "    min_minutes_delay_arr_1 = (min(when(col('year') == int(year_1), col('min_minutes_delay_arr')).otherwise(np.nan))).alias('min_minutes_delay_arr_year1')\n",
    "    min_minutes_delay_arr_2 = (min(when(col('year') == int(year_2), col('min_minutes_delay_arr')).otherwise(np.nan))).alias('min_minutes_delay_arr_year2')\n",
    "    min_minutes_delay_arr_3 = (min(when(col('year') == int(year_3), col('min_minutes_delay_arr')).otherwise(np.nan))).alias('min_minutes_delay_arr_year3')\n",
    "    total_minutes_delay_arr_1 = (sum(when(col('year') == int(year_1), col('max_minutes_delay_arr')).otherwise(0))).alias('total_minutes_delay_arr_year1')\n",
    "    total_minutes_delay_arr_2 = (sum(when(col('year') == int(year_2), col('max_minutes_delay_arr')).otherwise(0))).alias('total_minutes_delay_arr_year2')\n",
    "    total_minutes_delay_arr_3 = (sum(when(col('year') == int(year_3), col('max_minutes_delay_arr')).otherwise(0))).alias('total_minutes_delay_arr_year3')\n",
    "    \n",
    "    return (max_minutes_delay_arr_1, max_minutes_delay_arr_2, max_minutes_delay_arr_3, min_minutes_delay_arr_1, min_minutes_delay_arr_2,\n",
    "            min_minutes_delay_arr_3, total_minutes_delay_arr_1, total_minutes_delay_arr_2, total_minutes_delay_arr_3)\n",
    "\n",
    "def features_mkt_permission() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about marketing permission\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    mkt_permission = max(col('mkt_permission')).alias('mkt_permission')\n",
    "    \n",
    "    return mkt_permission\n",
    "\n",
    "def features_type_travel() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about number of not travelled, refunded\n",
    "    and exchanges\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_exchanges = first(col('n_exchanges')).alias('n_exchanges')\n",
    "    n_not_travelled = first(col('n_not_travelled')).alias('n_not_travelled')\n",
    "    n_refunded = first(col('n_refunded')).alias('n_refunded')\n",
    "\n",
    "    return n_exchanges, n_not_travelled, n_refunded\n",
    "\n",
    "def features_ods() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about number of ods\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    most_od_bought = (max(col('n_ods'))/first(col('n_ods_total'))).alias('most_od_bought')\n",
    "    n_destination = first('n_destination_city_od').alias('n_destination_city_od')\n",
    "    most_od_flown = first('most_od_flown').alias('most_od_flown')\n",
    "    point_of_sale = first(col('point_of_sale')).alias('point_of_sale')\n",
    "\n",
    "    return most_od_bought, n_destination, most_od_flown, point_of_sale\n",
    "\n",
    "def sales_channel() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about sale channel\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dir_ind = (sum(col('ind_direct_sale'))/sum(when((col('ind_direct_sale').isNotNull()), lit(1)).otherwise(0))).alias('ind_direct_sale')\n",
    "    \n",
    "    return dir_ind\n",
    "\n",
    "def features_nps() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about nps\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nps_100 = mean(col('nps_100')).alias('nps_100')\n",
    "    \n",
    "    return nps_100\n",
    "\n",
    "def features_op_carrier() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about IB_pctg\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    IB_pctg = first(col('IB_pctg')).alias('IB_pctg')\n",
    "    \n",
    "    return IB_pctg\n",
    "\n",
    "def features_perc_email_agency() -> object:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about IB_pctg\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Functions and conditions for grouping by at cid level\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    perc_email_agency = (sum(col('perc_email_agency'))/sum(when((col('perc_email_agency').isNotNull()), lit(1)).otherwise(0))).alias('perc_email_agency')\n",
    "    n_ticket_email = first(col('n_ticket_email')).alias('n_ticket_email')\n",
    "    \n",
    "    return perc_email_agency, n_ticket_email\n",
    "\n",
    "def create_final_features(df_ticket: DataFrame, df_future: DataFrame, date_train: str, po_execution: bool) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Function that creates conditions about marketing permission\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_customer_data_filtered_ticket: training dataframe at pnr/cid level\n",
    "    df_customer_data_filtered_future: dataframe for labeling\n",
    "    date_train: string with limit date for training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Final pyspark Dataframe with all features and cid for training\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    ## Extract features for groupby\n",
    "    year_1, year_2, year_3 = -1, -2, -3\n",
    "    # Frequency\n",
    "    (f_total, f_3, f_2, f_1, last_purchase, last2_purchase, last3_purchase,\n",
    "            last_flight, purchases_2015, purchases_2016, purchases_2017) = features_frequency(year_1, year_2, year_3, date_train)\n",
    "    # Gross Revenue\n",
    "    (gr_total, gr_3, gr_2, gr_1, gr_anc_total, gr_anc_3, gr_anc_2, \n",
    "    gr_anc_1, last_spent, gr_1_sh, gr_1_mh, gr_1_lh) = features_gross_revenue(year_1, year_2, year_3)\n",
    "    # Number of ancillaries\n",
    "    n_anc_total, n_anc_3, n_anc_2, n_anc_1, n_bags_total, n_seats_total, n_upgs_total, n_others_total = features_number_ancillaries(year_1, year_2, year_3)\n",
    "    # Euros in ancillaries\n",
    "    eur_anc_total, eur_anc_3, eur_anc_2, eur_anc_1, eur_bags_total, eur_seats_total, eur_upgs_total, eur_others_total = features_euros_ancillaries(year_1, year_2, year_3)\n",
    "    # Number of days anticipation\n",
    "    n_anticip_total, n_anticip_3, n_anticip_2, n_anticip_1 = features_days_anticipation(year_1, year_2, year_3)\n",
    "    # Number of hours in destination\n",
    "    hours_dest_total, hours_dest_3, hours_dest_2, hours_dest_1 = features_hours_destination(year_1, year_2, year_3)\n",
    "    # Number of hours in destination\n",
    "    ind_reason_total, ind_reason_3, ind_reason_2, ind_reason_1 = features_reason_business(year_1, year_2, year_3)\n",
    "    # Percentage of flights with departure at weekend\n",
    "    dep_weekend_total, dep_weekend_3, dep_weekend_2, dep_weekend_1 = features_dep_weekend(year_1, year_2, year_3)\n",
    "    # Percentage of OW flights\n",
    "    ow_flight_total, ow_flight_3, ow_flight_2, ow_flight_1 = features_ow_flights(year_1, year_2, year_3)\n",
    "    # IBPlus flag\n",
    "    ibplus, is_corporate, revenue_avios = features_ibplus()\n",
    "    # Number of passengers\n",
    "    alone_flights, couple_flights, group_flights = features_n_passengers()\n",
    "    # Type Indicator passenger (C, I ,A)\n",
    "    type_ind, is_resident = features_type_ind()\n",
    "    # Age\n",
    "    age = features_age()\n",
    "    # Season flight percentage\n",
    "    summer_flights, winter_flights, fall_flights, spring_flights = features_season()\n",
    "    # Seniority\n",
    "    seniority = features_seniority()\n",
    "    # Haul percentage\n",
    "    sh_haul, mh_haul, lh_haul = features_haul()\n",
    "    # Class code\n",
    "    economy, premium, business = features_class_code()\n",
    "    # Tier\n",
    "    ff_tier = features_tier()\n",
    "    # Flight spent\n",
    "    dev_spent_flight = features_flight_spent()\n",
    "    # Disruptions\n",
    "    (flag_misconnection_misc_last_week, flag_dng_last_week, flight_cnld_last_week,\n",
    "     flight_delay_last_week, flag_last_flight_delay) = features_disruptions()\n",
    "    # Delays\n",
    "    (max_minutes_delay_arr_1, max_minutes_delay_arr_2, max_minutes_delay_arr_3, min_minutes_delay_arr_1, min_minutes_delay_arr_2,\n",
    "     min_minutes_delay_arr_3, total_minutes_delay_arr_1, total_minutes_delay_arr_2, total_minutes_delay_arr_3) = features_flight_time(year_1, year_2, year_3)\n",
    "    # Mkt permission\n",
    "    mkt_permission = features_mkt_permission()\n",
    "    # Exchanges and travelled\n",
    "    n_exchanges, n_not_travelled, n_refunded = features_type_travel()\n",
    "    # Features ods\n",
    "    most_od_bought, n_destination, most_od_flown, point_of_sale = features_ods()\n",
    "    # Sales channel\n",
    "    dir_ind = sales_channel()\n",
    "    # OP Carrier code percentage\n",
    "    IB_pctg = features_op_carrier()\n",
    "    # Flag email agencie\n",
    "    perc_email_agency, n_ticket_email = features_perc_email_agency()\n",
    "    \n",
    "    # Groupby all features by id golden record\n",
    "    df_ticket_year = df_ticket.groupby('cid').agg(f_total, gr_total, n_anc_total, eur_anc_total,\n",
    "                                                 n_anticip_total, hours_dest_total, ind_reason_total,\n",
    "                                                 dep_weekend_total, ow_flight_total, gr_anc_total, gr_anc_3, gr_anc_2,\n",
    "                                                 gr_anc_1, f_3, f_2, f_1, ibplus, type_ind, age, alone_flights,\n",
    "                                                 couple_flights, group_flights, summer_flights, winter_flights,\n",
    "                                                 fall_flights, spring_flights, seniority, last_purchase, last2_purchase,\n",
    "                                                 last3_purchase, last_flight, sh_haul, mh_haul, lh_haul, economy, premium,\n",
    "                                                 business, ff_tier, dev_spent_flight,\n",
    "                                                 max_minutes_delay_arr_1, mkt_permission, n_exchanges, n_not_travelled,\n",
    "                                                 n_refunded, purchases_2015, purchases_2016, purchases_2017,\n",
    "                                                 most_od_bought, most_od_flown, dir_ind, last_spent, n_destination, is_corporate, revenue_avios,\n",
    "                                                 flag_misconnection_misc_last_week, flag_dng_last_week, flight_cnld_last_week,\n",
    "                                                 flight_delay_last_week, IB_pctg, is_resident, flag_last_flight_delay, perc_email_agency, n_ticket_email,\n",
    "                                                 gr_1_sh, gr_1_mh, gr_1_lh, point_of_sale)\n",
    "    \n",
    "    # Filter out clients with more than one purchase or more than one purchase + at least one purchase in year_mean\n",
    "    df_features = df_ticket_year.where((col('frequency_year1') > 0) & ((col('frequency_year1') + col('frequency_year2') + col('frequency_year3')) > 1))\n",
    "    # Add target gross revenue future\n",
    "    if not po_execution:\n",
    "        df_features = df_features.join(df_future, df_features[\"cid\"] == df_future[\"cid2\"], \"left_outer\")\n",
    "        df_features = df_features.fillna(value=0, subset=['gross_revenue_future'])\n",
    "        df_features = df_features.drop('cid2')\n",
    "    # Extract post features group\n",
    "    df_features = post_features_extraction(df_features)\n",
    "    # Churn label\n",
    "    if not po_execution:\n",
    "        df_features = df_features.withColumn('churn', when(col('gross_revenue_future') == 0, lit(1)).otherwise(lit(0)))\n",
    "    \n",
    "    # Replace np.inf, string AAAAAA by nan\n",
    "    df_features = df_features.replace(-np.inf, np.nan)\n",
    "    df_features = df_features.replace(np.inf, np.nan)\n",
    "    df_features = df_features.na.fill(value=0,subset=[\"purchases_2015\", \"purchases_2016\", \"purchases_2017\", \"revenue_avios\"])\n",
    "    \n",
    "    # Eliminate possible agencies and isolated CIDs\n",
    "    df_features = df_features.where(~(((col('perc_email_agency') > 0.7) | (col('ind_direct_sale') < 0.2)) & (col('frequency') < 4) & (col('n_ticket_email') > 1000)))\n",
    "\n",
    "    return df_features\n",
    "\n",
    "def execute_etl_churn(s3_dir_customer: str, columns: list, po_execution: bool=True) -> DataFrame:\n",
    "    \n",
    "    \"\"\"Function execute all pipeline for etl churn features\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    s3_dir_customer: s3 path for reading data\n",
    "    columns: columns for reading\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with all features\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if po_execution:\n",
    "        year_fin = 2023\n",
    "        month_fin = 12\n",
    "        day_fin = 31\n",
    "        date_future = None\n",
    "    else:\n",
    "        year_fin = 2022\n",
    "        month_fin = 12\n",
    "        day_fin = 31\n",
    "        date_future = datetime.datetime(year_fin, month_fin, day_fin)\n",
    "    if day_fin == 29 and month_fin == 2:\n",
    "        date_1year = datetime.datetime(year_fin - 1, month_fin, day_fin - 1)\n",
    "        date_2year = datetime.datetime(year_fin - 2, month_fin, day_fin - 1)\n",
    "        date_3year = datetime.datetime(year_fin - 3, month_fin, day_fin - 1)\n",
    "        date_limit = datetime.datetime(year_fin - 4, month_fin, day_fin - 1)\n",
    "    else:\n",
    "        date_1year = datetime.datetime(year_fin - 1, month_fin, day_fin)\n",
    "        date_2year = datetime.datetime(year_fin - 2, month_fin, day_fin)\n",
    "        date_3year = datetime.datetime(year_fin - 3, month_fin, day_fin)\n",
    "        date_limit = datetime.datetime(year_fin - 4, month_fin, day_fin)\n",
    "    date_train = str(date_1year).split()[0]\n",
    "    date_last_week = date_1year - datetime.timedelta(days=7)\n",
    "    print(date_limit, date_1year, date_future)\n",
    "\n",
    "    # Read data\n",
    "    df = read_s3_data_churn(s3_dir_customer, columns)\n",
    "    # Execute all code\n",
    "    df_prc, df_future = preprocess_data(df, date_train, date_1year, date_2year, date_3year, date_limit,\n",
    "                                        date_last_week, date_future)\n",
    "    df_customer_data_prc_features = create_final_features(df_prc, df_future, date_train, po_execution)\n",
    "    \n",
    "    return df_customer_data_prc_features, df_prc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31 00:00:00 2021-12-31 00:00:00 2022-12-31 00:00:00"
     ]
    }
   ],
   "source": [
    "# Data path from S3\n",
    "po_execution = False\n",
    "s3_dir_customer = 's3://iberia-data-lake/customer/customer_models_data_v2/'\n",
    "# s3_dir_customer_part = get_last_s3_partition(s3_dir_customer)\n",
    "s3_dir_customer_part = 's3://iberia-data-lake/customer/customer_models_data_v2/insert_date_ci=2023-02-19/'\n",
    "columns = ['cid', 'coupon_usage_code', 'prime_ticket_num', 'coupon_num', 'pnr_resiber', 'pnr_amadeus',\n",
    "           'gross_revenue_eur','ticket_sale_date', 'date_creation_pnr_resiber', 'num_bags', 'eur_bags', 'num_seats',\n",
    "           'eur_seats', 'num_upgs', 'eur_upgs', 'num_others', 'eur_others', 'ff_num',\n",
    "           'birth_date', 'pax_type_ind', 'num_days_anticipation', 'ind_reason_business',\n",
    "           'num_hours_in_destination', 'loc_dep_date', 'loc_arr_date', 'loc_arr_time', 'loc_dep_time',\n",
    "           'itinerary_od', 'date_creation_idgoldenrecord', 'haul', 'sold_class_code', 'ff_tier',\n",
    "           'pax_demand_space', 'ff_group', 'op_carrier_code', 'op_flight_num', 'flag_misconnection_misc', 'flag_dng', 'delayed_minutes_arrival',\n",
    "           'cancelled', 'mkt_permission', 'purchases_2015', 'purchases_2016', 'purchases_2017', 'ind_direct_sale', 'destination_city_od',\n",
    "           'is_corporate', 'revenue_avios', 'pax_type_seg', 'op_carrier_code', 'email_operative', 'point_of_sale']\n",
    "\n",
    "df_customer_data_prc_features, df_prc = execute_etl_churn(s3_dir_customer_part, columns, po_execution)\n",
    "df_prc = df_prc.cache()\n",
    "df_customer_data_prc_features = df_customer_data_prc_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901268"
     ]
    }
   ],
   "source": [
    "df_customer_data_prc_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>flag_ibplus</th><th>count</th></tr><tr><td>1</td><td>848377</td></tr><tr><td>0</td><td>1052891</td></tr></table><br /><pre></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "df_customer_data_prc_features.groupby('flag_ibplus').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>flag_ibplus_2</th><th>count</th></tr><tr><td>1</td><td>848377</td></tr><tr><td>0</td><td>1052891</td></tr></table><br /><pre></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "df_customer_data_prc_features.groupby('flag_ibplus_2').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>cid</th><th>frequency</th><th>gross_revenue_eur</th><th>n_ancillaries</th><th>eur_ancillaries</th><th>num_days_anticipation</th><th>num_hours_in_destination</th><th>ind_reason_business</th><th>loc_dep_weekend</th><th>ow_flights</th><th>gross_revenue_and_ancillaries</th><th>gross_revenue_and_ancillaries_year3</th><th>gross_revenue_and_ancillaries_year2</th><th>gross_revenue_and_ancillaries_year1</th><th>frequency_year3</th><th>frequency_year2</th><th>frequency_year1</th><th>flag_ibplus</th><th>flag_ibplus_2</th><th>pax_type_ind</th><th>age</th><th>alone_flights</th><th>couple_flights</th><th>group_flights</th><th>summer_flights</th><th>winter_flights</th><th>fall_flights</th><th>spring_flights</th><th>seniority</th><th>last_purchase</th><th>last2_purchase</th><th>last3_purchase</th><th>last_flight</th><th>sh_haul</th><th>mh_haul</th><th>lh_haul</th><th>economy</th><th>premium</th><th>business</th><th>ff_tier</th><th>mean_deviation_flight_spent</th><th>max_minutes_delay_arr_year1</th><th>mkt_permission</th><th>n_exchanges</th><th>n_not_travelled</th><th>n_refunded</th><th>purchases_2015</th><th>purchases_2016</th><th>purchases_2017</th><th>most_od_bought</th><th>most_od_flown</th><th>ind_direct_sale</th><th>last_spent</th><th>n_destination_city_od</th><th>is_corporate</th><th>revenue_avios</th><th>flag_misconnection_misc_last_week</th><th>flag_dng_last_week</th><th>flight_cnld_last_week</th><th>flight_delay_last_week</th><th>IB_pctg</th><th>is_resident</th><th>flag_last_flight_delay</th><th>perc_email_agency</th><th>n_ticket_email</th><th>gross_revenue_sh_year1</th><th>gross_revenue_mh_year1</th><th>gross_revenue_lh_year1</th><th>point_of_sale</th><th>gross_revenue_future</th><th>mean_spend</th><th>mean_spend_year3</th><th>mean_spend_year2</th><th>mean_spend_year1</th><th>churn</th></tr></table><br /><pre></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "df_customer_data_prc_features.where(col('flag_ibplus') != col('flag_ibplus_2')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_customer_data_prc_features.repartition(20).write.mode('overwrite').orc('s3://iberia-data-lake/customer/churn_model_v2/train_data_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-31 00:00:00 2022-12-31 00:00:00 None"
     ]
    }
   ],
   "source": [
    "# Data path from S3\n",
    "po_execution = True\n",
    "s3_dir_customer = 's3://iberia-data-lake/customer/customer_models_data_v2/'\n",
    "# s3_dir_customer_part = get_last_s3_partition(s3_dir_customer)\n",
    "s3_dir_customer_part = 's3://iberia-data-lake/customer/customer_models_data_v2/insert_date_ci=2023-02-19/'\n",
    "columns = ['cid', 'coupon_usage_code', 'prime_ticket_num', 'coupon_num', 'pnr_resiber', 'pnr_amadeus',\n",
    "           'gross_revenue_eur','ticket_sale_date', 'date_creation_pnr_resiber', 'num_bags', 'eur_bags', 'num_seats',\n",
    "           'eur_seats', 'num_upgs', 'eur_upgs', 'num_others', 'eur_others', 'ff_num',\n",
    "           'birth_date', 'pax_type_ind', 'num_days_anticipation', 'ind_reason_business',\n",
    "           'num_hours_in_destination', 'loc_dep_date', 'loc_arr_date', 'loc_arr_time', 'loc_dep_time',\n",
    "           'itinerary_od', 'date_creation_idgoldenrecord', 'haul', 'sold_class_code', 'ff_tier',\n",
    "           'pax_demand_space', 'ff_group', 'op_carrier_code', 'op_flight_num', 'flag_misconnection_misc', 'flag_dng', 'delayed_minutes_arrival',\n",
    "           'cancelled', 'mkt_permission', 'purchases_2015', 'purchases_2016', 'purchases_2017', 'ind_direct_sale', 'destination_city_od',\n",
    "           'is_corporate', 'revenue_avios', 'pax_type_seg', 'op_carrier_code', 'email_operative', 'point_of_sale']\n",
    "\n",
    "df_customer_data_prc_features, df_prc = execute_etl_churn(s3_dir_customer_part, columns, po_execution)\n",
    "df_prc = df_prc.cache()\n",
    "df_customer_data_prc_features = df_customer_data_prc_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139060"
     ]
    }
   ],
   "source": [
    "df_customer_data_prc_features.where(col('flag_ibplus') == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_customer_data_prc_features.repartition(20).write.mode('overwrite').orc('s3://iberia-data-lake/customer/churn_model_v2/po_data_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>cid</th><th>frequency</th><th>gross_revenue_eur</th><th>n_ancillaries</th><th>eur_ancillaries</th><th>num_days_anticipation</th><th>num_hours_in_destination</th><th>ind_reason_business</th><th>loc_dep_weekend</th><th>ow_flights</th><th>gross_revenue_and_ancillaries</th><th>gross_revenue_and_ancillaries_year3</th><th>gross_revenue_and_ancillaries_year2</th><th>gross_revenue_and_ancillaries_year1</th><th>frequency_year3</th><th>frequency_year2</th><th>frequency_year1</th><th>flag_ibplus</th><th>pax_type_ind</th><th>age</th><th>alone_flights</th><th>couple_flights</th><th>group_flights</th><th>summer_flights</th><th>winter_flights</th><th>fall_flights</th><th>spring_flights</th><th>seniority</th><th>last_purchase</th><th>last2_purchase</th><th>last3_purchase</th><th>last_flight</th><th>sh_haul</th><th>mh_haul</th><th>lh_haul</th><th>economy</th><th>premium</th><th>business</th><th>ff_tier</th><th>mean_deviation_flight_spent</th><th>max_minutes_delay_arr_year1</th><th>mkt_permission</th><th>n_exchanges</th><th>n_not_travelled</th><th>n_refunded</th><th>purchases_2015</th><th>purchases_2016</th><th>purchases_2017</th><th>most_od_bought</th><th>most_od_flown</th><th>ind_direct_sale</th><th>last_spent</th><th>n_destination_city_od</th><th>is_corporate</th><th>revenue_avios</th><th>flag_misconnection_misc_last_week</th><th>flag_dng_last_week</th><th>flight_cnld_last_week</th><th>flight_delay_last_week</th><th>IB_pctg</th><th>is_resident</th><th>flag_last_flight_delay</th><th>perc_email_agency</th><th>n_ticket_email</th><th>gross_revenue_sh_year1</th><th>gross_revenue_mh_year1</th><th>gross_revenue_lh_year1</th><th>point_of_sale</th><th>mean_spend</th><th>mean_spend_year3</th><th>mean_spend_year2</th><th>mean_spend_year1</th></tr><tr><td>10022361</td><td>2</td><td>570.0</td><td>1</td><td>74.74</td><td>10.5</td><td>2072.1666</td><td>0.5</td><td>0.0</td><td>0.5</td><td>644.74</td><td>0.0</td><td>564.74</td><td>80.0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>A</td><td>24.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>656</td><td>121</td><td>656</td><td>null</td><td>16</td><td>0.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.18078783420207964</td><td>-27.0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>VCE</td><td>0.5</td><td>80.0</td><td>2</td><td>0.5</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>3</td><td>0.0</td><td>80.0</td><td>0.0</td><td>ES</td><td>322.37</td><td>0.0</td><td>564.74</td><td>80.0</td></tr><tr><td>10048703</td><td>2</td><td>419.0</td><td>2</td><td>36.0</td><td>5.0</td><td>42.8333</td><td>0.0</td><td>0.5</td><td>0.0</td><td>455.0</td><td>0.0</td><td>188.0</td><td>267.0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>A</td><td>53.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>494</td><td>44</td><td>494</td><td>null</td><td>38</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>2</td><td>0.7316833302817913</td><td>-11.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>null</td><td>1.0</td><td>267.0</td><td>3</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>2</td><td>0.0</td><td>267.0</td><td>0.0</td><td>ES</td><td>227.5</td><td>0.0</td><td>188.0</td><td>267.0</td></tr><tr><td>10050017</td><td>2</td><td>66.0</td><td>0</td><td>0.0</td><td>62.0</td><td>null</td><td>0.0</td><td>1.0</td><td>1.0</td><td>66.0</td><td>0.0</td><td>0.0</td><td>66.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>26</td><td>26</td><td>26</td><td>null</td><td>-36</td><td>0.5</td><td>0.5</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.05666933367375418</td><td>18.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>1.0</td><td>18.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>2</td><td>48.0</td><td>18.0</td><td>0.0</td><td>NL</td><td>33.0</td><td>0.0</td><td>0.0</td><td>33.0</td></tr><tr><td>10056932</td><td>2</td><td>29.0</td><td>1</td><td>40.0</td><td>19.0</td><td>176.75</td><td>0.0</td><td>0.0</td><td>0.5</td><td>69.0</td><td>0.0</td><td>19.0</td><td>50.0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>721</td><td>56</td><td>721</td><td>null</td><td>31</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.7527035807438452</td><td>10.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>NTE</td><td>1.0</td><td>50.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>4</td><td>0.0</td><td>10.0</td><td>0.0</td><td>ES</td><td>34.5</td><td>0.0</td><td>19.0</td><td>50.0</td></tr><tr><td>10058857</td><td>3</td><td>192.44</td><td>0</td><td>0.0</td><td>35.333333333333336</td><td>null</td><td>0.6666666666666666</td><td>0.6666666666666666</td><td>1.0</td><td>192.44</td><td>0.0</td><td>33.8</td><td>158.64</td><td>0</td><td>1</td><td>2</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.3333333333333333</td><td>0.6666666666666666</td><td>0.0</td><td>0.0</td><td>380</td><td>196</td><td>339</td><td>380</td><td>166</td><td>0.6666666666666666</td><td>0.3333333333333333</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.03595035771176919</td><td>9.0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.6666666666666666</td><td>VGO</td><td>1.0</td><td>125.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.6666666666666666</td><td>0</td><td>0</td><td>0.0</td><td>4</td><td>33.64</td><td>125.0</td><td>0.0</td><td>ES</td><td>64.14666666666666</td><td>0.0</td><td>33.8</td><td>79.32</td></tr><tr><td>10071852</td><td>2</td><td>965.78</td><td>0</td><td>0.0</td><td>28.0</td><td>233.3333</td><td>1.0</td><td>0.0</td><td>0.0</td><td>965.78</td><td>0.0</td><td>0.0</td><td>965.78</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>28.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>133</td><td>133</td><td>135</td><td>null</td><td>95</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.6666666666666666</td><td>0.3333333333333333</td><td>0.0</td><td>0</td><td>NaN</td><td>-2.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>BRU</td><td>0.0</td><td>144.2</td><td>2</td><td>0.5</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.6666666666666666</td><td>0</td><td>0</td><td>0.0</td><td>4</td><td>0.0</td><td>0.0</td><td>965.78</td><td>CO</td><td>482.89</td><td>0.0</td><td>0.0</td><td>482.89</td></tr><tr><td>10117317</td><td>2</td><td>234.55</td><td>0</td><td>0.0</td><td>5.5</td><td>null</td><td>1.0</td><td>0.0</td><td>1.0</td><td>234.55</td><td>0.0</td><td>0.0</td><td>234.55</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.5</td><td>214</td><td>78</td><td>214</td><td>null</td><td>74</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.8643230537599548</td><td>17.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>1.0</td><td>180.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>1</td><td>0.0</td><td>3</td><td>234.55</td><td>0.0</td><td>0.0</td><td>ES</td><td>117.275</td><td>0.0</td><td>0.0</td><td>117.275</td></tr><tr><td>10117752</td><td>2</td><td>315.02</td><td>1</td><td>79.47</td><td>24.5</td><td>13.5</td><td>1.0</td><td>0.6666666666666666</td><td>0.5</td><td>394.49</td><td>0.0</td><td>0.0</td><td>394.49</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>24.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.5</td><td>1873</td><td>200</td><td>255</td><td>null</td><td>166</td><td>0.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.2484193077748118</td><td>3.0</td><td>1</td><td>0</td><td>2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0.5</td><td>MIL</td><td>1.0</td><td>379.49</td><td>3</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>7</td><td>0.0</td><td>15.0</td><td>300.02</td><td>US</td><td>197.245</td><td>0.0</td><td>0.0</td><td>197.245</td></tr><tr><td>10117972</td><td>5</td><td>511.59000000000003</td><td>0</td><td>0.0</td><td>23.8</td><td>86.708325</td><td>0.0</td><td>0.5555555555555556</td><td>0.2</td><td>511.59000000000003</td><td>0.0</td><td>0.0</td><td>511.59000000000003</td><td>0</td><td>0</td><td>5</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.2</td><td>0.2</td><td>0.0</td><td>0.6</td><td>288</td><td>42</td><td>203</td><td>241</td><td>-2</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.25241424000043</td><td>-2.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.8</td><td>PMI</td><td>1.0</td><td>154.53</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>0.0</td><td>9</td><td>511.59000000000003</td><td>0.0</td><td>0.0</td><td>ES</td><td>102.31800000000001</td><td>0.0</td><td>0.0</td><td>102.31800000000001</td></tr><tr><td>10120594</td><td>2</td><td>365.29999999999995</td><td>4</td><td>128.06</td><td>35.5</td><td>1450.1666500000001</td><td>0.0</td><td>0.25</td><td>0.0</td><td>493.36</td><td>0.0</td><td>172.92000000000002</td><td>320.44</td><td>0</td><td>1</td><td>1</td><td>0</td><td>A</td><td>55.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.5</td><td>677</td><td>160</td><td>677</td><td>null</td><td>32</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.6202216834043185</td><td>7.0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>MUC</td><td>1.0</td><td>320.44</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.75</td><td>0</td><td>0</td><td>0.0</td><td>2</td><td>0.0</td><td>0.0</td><td>243.01999999999998</td><td>MX</td><td>246.68</td><td>0.0</td><td>172.92000000000002</td><td>320.44</td></tr><tr><td>10123756</td><td>2</td><td>426.0</td><td>1</td><td>162.0</td><td>14.5</td><td>null</td><td>0.0</td><td>0.5</td><td>1.0</td><td>588.0</td><td>58.0</td><td>0.0</td><td>530.0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>A</td><td>null</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>854</td><td>128</td><td>854</td><td>null</td><td>118</td><td>0.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.16341257124477204</td><td>NaN</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>1.0</td><td>530.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>10</td><td>0.0</td><td>0.0</td><td>368.0</td><td>ES</td><td>294.0</td><td>58.0</td><td>0.0</td><td>530.0</td></tr><tr><td>10138967</td><td>2</td><td>333.0</td><td>0</td><td>0.0</td><td>1.0</td><td>256.5833</td><td>0.0</td><td>0.3333333333333333</td><td>0.5</td><td>333.0</td><td>0.0</td><td>0.0</td><td>333.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>149</td><td>146</td><td>149</td><td>null</td><td>138</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.03218854884141041</td><td>167.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>0.0</td><td>75.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>1</td><td>0.0</td><td>6</td><td>0.0</td><td>333.0</td><td>0.0</td><td>ES</td><td>166.5</td><td>0.0</td><td>0.0</td><td>166.5</td></tr><tr><td>10139045</td><td>2</td><td>80.0</td><td>0</td><td>0.0</td><td>2.5</td><td>null</td><td>0.0</td><td>0.5</td><td>1.0</td><td>80.0</td><td>0.0</td><td>0.0</td><td>80.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>170</td><td>164</td><td>170</td><td>null</td><td>162</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.01873321304553...</td><td>-20.0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>ZRH</td><td>0.0</td><td>67.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>6</td><td>0.0</td><td>80.0</td><td>0.0</td><td>ES</td><td>40.0</td><td>0.0</td><td>0.0</td><td>40.0</td></tr><tr><td>10151122</td><td>2</td><td>221.78</td><td>0</td><td>0.0</td><td>40.0</td><td>77.1666</td><td>0.0</td><td>0.6666666666666666</td><td>0.5</td><td>221.78</td><td>127.0</td><td>0.0</td><td>94.78</td><td>1</td><td>0</td><td>1</td><td>0</td><td>A</td><td>null</td><td>0.0</td><td>0.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>955</td><td>52</td><td>957</td><td>null</td><td>25</td><td>0.5</td><td>0.5</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.47120709212404144</td><td>-7.0</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>SPC</td><td>0.5</td><td>94.78</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>8</td><td>94.78</td><td>0.0</td><td>0.0</td><td>ES</td><td>110.89</td><td>127.0</td><td>0.0</td><td>94.78</td></tr><tr><td>10157100</td><td>4</td><td>765.0</td><td>4</td><td>50.0</td><td>9.5</td><td>62.4583</td><td>0.5</td><td>0.6666666666666666</td><td>0.5</td><td>815.0</td><td>601.0</td><td>0.0</td><td>214.0</td><td>3</td><td>0</td><td>1</td><td>1</td><td>A</td><td>34.0</td><td>0.25</td><td>0.25</td><td>0.5</td><td>0.0</td><td>0.25</td><td>0.0</td><td>0.75</td><td>1002</td><td>311</td><td>978</td><td>978</td><td>295</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.06373687685696122</td><td>-14.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>BIO</td><td>0.0</td><td>214.0</td><td>3</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.8181818181818182</td><td>0</td><td>0</td><td>0.0</td><td>6</td><td>0.0</td><td>214.0</td><td>0.0</td><td>IT</td><td>203.75</td><td>200.33333333333334</td><td>0.0</td><td>214.0</td></tr><tr><td>10187213</td><td>2</td><td>62.0</td><td>2</td><td>50.0</td><td>21.0</td><td>null</td><td>0.0</td><td>0.5</td><td>1.0</td><td>112.0</td><td>0.0</td><td>0.0</td><td>112.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>256</td><td>236</td><td>256</td><td>null</td><td>222</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.44410603284460015</td><td>-2.0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>1.0</td><td>61.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>2</td><td>0.0</td><td>62.0</td><td>0.0</td><td>DE</td><td>56.0</td><td>0.0</td><td>0.0</td><td>56.0</td></tr><tr><td>10198524</td><td>3</td><td>693.0</td><td>1</td><td>6.0</td><td>328.0</td><td>84.5833</td><td>0.6666666666666666</td><td>0.5</td><td>0.0</td><td>699.0</td><td>311.0</td><td>174.0</td><td>214.0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>A</td><td>null</td><td>0.6666666666666666</td><td>0.0</td><td>0.3333333333333333</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>913</td><td>178</td><td>546</td><td>913</td><td>-133</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.3555728440006374</td><td>2.0</td><td>0</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>SVQ</td><td>1.0</td><td>214.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>216</td><td>0.0</td><td>208.0</td><td>0.0</td><td>DE</td><td>233.0</td><td>311.0</td><td>174.0</td><td>214.0</td></tr><tr><td>10199082</td><td>2</td><td>518.0</td><td>1</td><td>140.0</td><td>12.0</td><td>null</td><td>1.0</td><td>0.0</td><td>1.0</td><td>658.0</td><td>0.0</td><td>483.0</td><td>175.0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>A</td><td>50.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>408</td><td>172</td><td>408</td><td>null</td><td>154</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>NaN</td><td>-28.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>0.0</td><td>175.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>2</td><td>0.0</td><td>0.0</td><td>175.0</td><td>ES</td><td>329.0</td><td>0.0</td><td>483.0</td><td>175.0</td></tr><tr><td>10203311</td><td>2</td><td>86.2</td><td>0</td><td>0.0</td><td>17.0</td><td>null</td><td>0.0</td><td>0.5</td><td>1.0</td><td>86.2</td><td>0.0</td><td>0.0</td><td>86.2</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>130</td><td>130</td><td>130</td><td>null</td><td>111</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.12561453616407003</td><td>-5.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>TCI</td><td>0.0</td><td>27.53</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>19</td><td>86.2</td><td>0.0</td><td>0.0</td><td>HU</td><td>43.1</td><td>0.0</td><td>0.0</td><td>43.1</td></tr><tr><td>10206095</td><td>2</td><td>411.0</td><td>1</td><td>75.0</td><td>71.5</td><td>3160.4166</td><td>0.5</td><td>0.3333333333333333</td><td>0.5</td><td>486.0</td><td>270.0</td><td>0.0</td><td>216.0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>A</td><td>27.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.5</td><td>1151</td><td>154</td><td>737</td><td>null</td><td>46</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.04235848460069219</td><td>NaN</td><td>1</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>LIM</td><td>1.0</td><td>216.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>3</td><td>0.0</td><td>0.0</td><td>216.0</td><td>ES</td><td>243.0</td><td>270.0</td><td>0.0</td><td>216.0</td></tr><tr><td>10218949</td><td>2</td><td>425.0</td><td>0</td><td>0.0</td><td>15.5</td><td>84.24995</td><td>0.0</td><td>0.5</td><td>0.0</td><td>425.0</td><td>0.0</td><td>0.0</td><td>425.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>113</td><td>29</td><td>113</td><td>null</td><td>-3</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.4222246167722109</td><td>-6.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>null</td><td>1.0</td><td>221.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>0.0</td><td>4</td><td>425.0</td><td>0.0</td><td>0.0</td><td>ES</td><td>212.5</td><td>0.0</td><td>0.0</td><td>212.5</td></tr><tr><td>10227709</td><td>2</td><td>303.95</td><td>0</td><td>0.0</td><td>62.0</td><td>null</td><td>0.0</td><td>0.5</td><td>1.0</td><td>303.95</td><td>203.53</td><td>0.0</td><td>100.42</td><td>1</td><td>0</td><td>1</td><td>1</td><td>A</td><td>36.0</td><td>0.0</td><td>0.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>968</td><td>119</td><td>967</td><td>null</td><td>94</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1</td><td>-0.3764376829409525</td><td>-30.0</td><td>1</td><td>3</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>1.0</td><td>100.42</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>5</td><td>0.0</td><td>0.0</td><td>100.42</td><td>AR</td><td>151.975</td><td>203.53</td><td>0.0</td><td>100.42</td></tr><tr><td>10234194</td><td>2</td><td>86.3</td><td>1</td><td>8.0</td><td>46.0</td><td>null</td><td>0.0</td><td>0.6666666666666666</td><td>0.5</td><td>94.3</td><td>0.0</td><td>84.0</td><td>10.3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>512</td><td>147</td><td>512</td><td>null</td><td>104</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.4975132347717943</td><td>-5.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>TCI</td><td>1.0</td><td>10.3</td><td>3</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>3</td><td>10.3</td><td>0.0</td><td>0.0</td><td>ES</td><td>47.15</td><td>0.0</td><td>84.0</td><td>10.3</td></tr><tr><td>10248105</td><td>2</td><td>269.0</td><td>0</td><td>0.0</td><td>53.5</td><td>null</td><td>0.0</td><td>0.5</td><td>1.0</td><td>269.0</td><td>0.0</td><td>0.0</td><td>269.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>14.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>205</td><td>205</td><td>205</td><td>null</td><td>126</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.14809401567897776</td><td>6.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>RAK</td><td>0.0</td><td>79.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>16</td><td>0.0</td><td>269.0</td><td>0.0</td><td>ES</td><td>134.5</td><td>0.0</td><td>0.0</td><td>134.5</td></tr><tr><td>10249085</td><td>2</td><td>192.0</td><td>0</td><td>0.0</td><td>6.0</td><td>null</td><td>0.5</td><td>0.0</td><td>1.0</td><td>192.0</td><td>0.0</td><td>0.0</td><td>192.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>294</td><td>294</td><td>294</td><td>null</td><td>288</td><td>0.5</td><td>0.5</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.35298946504375506</td><td>2.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>MAD</td><td>1.0</td><td>123.0</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>2</td><td>123.0</td><td>69.0</td><td>0.0</td><td>BE</td><td>96.0</td><td>0.0</td><td>0.0</td><td>96.0</td></tr><tr><td>10250248</td><td>3</td><td>525.96</td><td>0</td><td>0.0</td><td>24.333333333333332</td><td>57.4166</td><td>0.0</td><td>0.75</td><td>0.6666666666666666</td><td>525.96</td><td>337.96000000000004</td><td>0.0</td><td>188.0</td><td>1</td><td>0</td><td>2</td><td>0</td><td>A</td><td>25.0</td><td>0.0</td><td>0.3333333333333333</td><td>0.6666666666666666</td><td>0.3333333333333333</td><td>0.0</td><td>0.6666666666666666</td><td>0.0</td><td>1572</td><td>49</td><td>49</td><td>908</td><td>38</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.36988405915975675</td><td>NaN</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0.3333333333333333</td><td>TCI</td><td>0.6666666666666666</td><td>136.0</td><td>3</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>0.0</td><td>8</td><td>188.0</td><td>0.0</td><td>0.0</td><td>ES</td><td>175.32000000000002</td><td>337.96000000000004</td><td>0.0</td><td>94.0</td></tr><tr><td>10254277</td><td>2</td><td>210.0</td><td>2</td><td>33.0</td><td>23.5</td><td>107.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>243.0</td><td>0.0</td><td>0.0</td><td>243.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>22.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>70</td><td>49</td><td>70</td><td>null</td><td>24</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.21959821487082293</td><td>25.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>PAR</td><td>1.0</td><td>195.0</td><td>3</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>4</td><td>0.0</td><td>210.0</td><td>0.0</td><td>ES</td><td>121.5</td><td>0.0</td><td>0.0</td><td>121.5</td></tr><tr><td>10267632</td><td>2</td><td>99.09</td><td>0</td><td>0.0</td><td>27.0</td><td>49.0833</td><td>0.0</td><td>0.6666666666666666</td><td>0.5</td><td>99.09</td><td>0.0</td><td>44.54</td><td>54.55</td><td>0</td><td>1</td><td>1</td><td>0</td><td>A</td><td>43.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.5</td><td>1676</td><td>192</td><td>696</td><td>null</td><td>190</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.08081597026397457</td><td>-3.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.0</td><td>AGP</td><td>1.0</td><td>54.55</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>0</td><td>0</td><td>0.0</td><td>4</td><td>54.55</td><td>0.0</td><td>0.0</td><td>ES</td><td>49.545</td><td>0.0</td><td>44.54</td><td>54.55</td></tr><tr><td>10267659</td><td>2</td><td>83.0</td><td>2</td><td>110.0</td><td>13.0</td><td>null</td><td>0.0</td><td>0.0</td><td>1.0</td><td>193.0</td><td>0.0</td><td>0.0</td><td>193.0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>A</td><td>null</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>144</td><td>112</td><td>144</td><td>null</td><td>115</td><td>0.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>-0.5692414371486395</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1.0</td><td>DKR</td><td>0.0</td><td>158.12</td><td>1</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0</td><td>0</td><td>0.0</td><td>8</td><td>0.0</td><td>15.0</td><td>68.0</td><td>FR</td><td>96.5</td><td>0.0</td><td>0.0</td><td>96.5</td></tr><tr><td>10269932</td><td>3</td><td>200.91</td><td>0</td><td>0.0</td><td>7.333333333333333</td><td>null</td><td>0.0</td><td>0.0</td><td>1.0</td><td>200.91</td><td>0.0</td><td>75.46000000000001</td><td>125.45</td><td>0</td><td>2</td><td>1</td><td>0</td><td>A</td><td>30.0</td><td>0.6666666666666666</td><td>0.3333333333333333</td><td>0.0</td><td>0.6666666666666666</td><td>0.3333333333333333</td><td>0.0</td><td>0.0</td><td>494</td><td>189</td><td>369</td><td>494</td><td>184</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0.2846666553965191</td><td>-11.0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.6666666666666666</td><td>LEI</td><td>1.0</td><td>125.45</td><td>2</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>5</td><td>125.45</td><td>0.0</td><td>0.0</td><td>ES</td><td>66.97</td><td>0.0</td><td>37.730000000000004</td><td>125.45</td></tr></table><br /><pre>only showing top 30 rows</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "df_customer_data_prc_features.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "PySpark (SparkMagic)",
   "language": "python",
   "name": "pysparkkernel__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-sparkmagic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "lcc_arn": "arn:aws:sagemaker:eu-west-1:077156906314:studio-lifecycle-config/startup-create-emr-spark-cluster-plazaroh"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
