{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fba4cd-3f9d-40bf-b9ae-6b031bf8b581",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting boto3==1.19.12\n",
      "  Using cached boto3-1.19.12-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting botocore<1.23.0,>=1.22.12 (from boto3==1.19.12)\n",
      "  Using cached botocore-1.22.12-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3==1.19.12) (0.10.0)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0 (from boto3==1.19.12)\n",
      "  Using cached s3transfer-0.5.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.23.0,>=1.22.12->boto3==1.19.12) (2.8.2)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.23.0,>=1.22.12->boto3==1.19.12)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.12->boto3==1.19.12) (1.16.0)\n",
      "Using cached boto3-1.19.12-py3-none-any.whl (131 kB)\n",
      "Using cached botocore-1.22.12-py3-none-any.whl (8.1 MB)\n",
      "Using cached s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.33.9\n",
      "    Uninstalling botocore-1.33.9:\n",
      "      Successfully uninstalled botocore-1.33.9\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.8.2\n",
      "    Uninstalling s3transfer-0.8.2:\n",
      "      Successfully uninstalled s3transfer-0.8.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.33.9\n",
      "    Uninstalling boto3-1.33.9:\n",
      "      Successfully uninstalled boto3-1.33.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.31.9 requires botocore==1.33.9, but you have botocore 1.22.12 which is incompatible.\n",
      "awscli 1.31.9 requires s3transfer<0.9.0,>=0.8.0, but you have s3transfer 0.5.2 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.19.12 which is incompatible.\n",
      "sagemaker-studio-analytics-extension 0.0.20 requires boto3<2.0,>=1.26.49, but you have boto3 1.19.12 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.19.12 botocore-1.22.12 s3transfer-0.5.2 urllib3-1.26.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.10/site-packages (from s3fs) (1.22.12)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2022.7.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.26.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.4)\n",
      "Using cached lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting shap\n",
      "  Using cached shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from shap) (1.26.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from shap) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from shap) (2.1.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.10/site-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.10/site-packages (from shap) (21.3)\n",
      "Collecting slicer==0.0.7 (from shap)\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap) (0.55.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>20.9->shap) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.10/site-packages (from numba->shap) (0.38.0)\n",
      "Collecting numpy (from shap)\n",
      "  Using cached numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from numba->shap) (69.0.2)\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas (from shap)\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "  Using cached pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "  Using cached pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "  Using cached pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "  Using cached pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "INFO: pip is still looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Using cached shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
      "Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Using cached numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "Using cached pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Installing collected packages: slicer, numpy, pandas, shap\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.3\n",
      "    Uninstalling pandas-2.1.3:\n",
      "      Successfully uninstalled pandas-2.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "astropy 6.0.0 requires numpy<2,>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.2 which is incompatible.\n",
      "sagemaker 2.199.0 requires boto3<2.0,>=1.33.3, but you have boto3 1.19.12 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.21.6 pandas-2.0.3 shap-0.45.0 slicer-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting catboost\n",
      "  Using cached catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.11.4)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.9.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.0.1)\n",
      "Using cached catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.3 graphviz-0.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install boto3==1.19.12\n",
    "!pip install s3fs\n",
    "!pip install lightgbm\n",
    "!pip install shap\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af85a46-bdbe-42b9-acb1-d3003c753d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from datetime import datetime, timedelta\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import os\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "import datetime\n",
    "import boto3\n",
    "import s3fs\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Models\n",
    "from catboost import CatBoostClassifier, cv, Pool\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "# Random\n",
    "import random\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69292ce9-c093-46e6-a72b-42831c010179",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23a55f7a-29b1-4abe-a32a-343668467098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def calculate_SHAP_and_probability_binary(model_promoter, model_detractor, df):\n",
    "    # Extraer ID y fechas, manteniendo el índice\n",
    "    id_df = df[['respondent_id', 'date_flight_local']]\n",
    "    \n",
    "    # Preparar el conjunto de datos para predicciones, excluyendo ID y fechas\n",
    "    test_set = df.drop(['respondent_id', 'date_flight_local'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Predicciones y probabilidades para promotores\n",
    "    promoter_test_set = test_set.drop(['promoter_binary'], axis=1, errors='ignore')\n",
    "    predictions_promoter = pd.DataFrame(model_promoter.predict(promoter_test_set), index=promoter_test_set.index, columns=[\"prediction_prom\"])\n",
    "    proba_promoter = pd.DataFrame(model_promoter.predict_proba(promoter_test_set)[:, 1], index=promoter_test_set.index, columns=[\"out_prob_prom\"])\n",
    "    \n",
    "    # Predicciones y probabilidades para detractores\n",
    "    detractor_test_set = test_set.drop(['detractor_binary'], axis=1, errors='ignore')\n",
    "    predictions_detractor = pd.DataFrame(model_detractor.predict(detractor_test_set), index=detractor_test_set.index, columns=[\"prediction_det\"])\n",
    "    proba_detractor = pd.DataFrame(model_detractor.predict_proba(detractor_test_set)[:, 1], index=detractor_test_set.index, columns=[\"out_prob_det\"])\n",
    "    \n",
    "    # Combinar resultados de predicción, manteniendo el índice original\n",
    "    prediction = pd.concat([id_df, test_set, predictions_promoter, proba_promoter, predictions_detractor, proba_detractor], axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo promotor\n",
    "    shap_Explainer_promoter = shap.TreeExplainer(model_promoter)\n",
    "    shap_values_promoter = shap_Explainer_promoter.shap_values(promoter_test_set)\n",
    "    feature_names = [i for i in promoter_test_set.columns]\n",
    "    shap_values_prom = pd.DataFrame(shap_values_promoter, index=promoter_test_set.index, columns=[f\"{i}_prom\" for i in feature_names])\n",
    "    shap_values_prom[\"base_value_prom\"] = shap_Explainer_promoter.expected_value\n",
    "    shap_values_prom[\"out_value_prom\"] = shap_values_prom.sum(axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo detractor\n",
    "    shap_Explainer_detractor = shap.TreeExplainer(model_detractor)\n",
    "    shap_values_detractor = shap_Explainer_detractor.shap_values(detractor_test_set)\n",
    "    shap_values_det = pd.DataFrame(shap_values_detractor, index=detractor_test_set.index, columns=[f\"{i}_det\" for i in feature_names])\n",
    "    shap_values_det[\"base_value_det\"] = shap_Explainer_detractor.expected_value\n",
    "    shap_values_det[\"out_value_det\"] = shap_values_det.sum(axis=1)\n",
    "    \n",
    "    # Combinar SHAP values con predicciones, manteniendo el índice original\n",
    "    output_df = pd.concat([prediction, shap_values_prom, shap_values_det], axis=1)\n",
    "    \n",
    "    # Devolver el dataframe de salida\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [col for col in df.columns if col.endswith(class_suffix)]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    \n",
    "    # Convertir el valor base a probabilidades y actualizar el nombre de la columna\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "    \n",
    "    # Convertir valores SHAP a probabilidades sin cambiar los nombres de las columnas\n",
    "    for col in shap_columns:\n",
    "        output_df[col] = inv_logit(output_df[col])\n",
    "    \n",
    "    # Asegurarse de incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = ['respondent_id', 'date_flight_local'] + shap_columns + [f'base_prob{class_suffix}'] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    return output_df\n",
    "\n",
    "def adjust_shap_values_binary(shap_values, base_prob, out_prob):\n",
    "    \"\"\"Ajustar los valores SHAP para un modelo binario basado en la distancia.\"\"\"\n",
    "    # Calcular la distancia total deseada entre la probabilidad base y la de salida\n",
    "    total_distance = out_prob - base_prob\n",
    "    # Calcular la suma total de los valores SHAP\n",
    "    total_shap = np.sum(shap_values)\n",
    "    # Calcular el factor de ajuste si la suma total de SHAP no es cero\n",
    "    adjustment_factor = total_distance / total_shap if total_shap != 0 else 0\n",
    "    # Ajustar los valores SHAP\n",
    "    return shap_values * adjustment_factor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [f'{feature}{class_suffix}' for feature in features_dummy if f'{feature}{class_suffix}' in df.columns]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    out_prob_col = f'out_prob{class_suffix}'\n",
    "\n",
    "    # Calcular la probabilidad base usando softmax o inv_logit según sea apropiado\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "\n",
    "    for index, row in output_df.iterrows():\n",
    "        # Extraer los valores SHAP para ajustar\n",
    "        shap_values = row[shap_columns].values\n",
    "        # Calcular los valores SHAP ajustados\n",
    "        adjusted_shap_values = adjust_shap_values_binary(shap_values, row[f'base_prob{class_suffix}'], row[out_prob_col])\n",
    "        # Actualizar el DataFrame con los valores SHAP ajustados\n",
    "        output_df.loc[index, shap_columns] = adjusted_shap_values\n",
    "\n",
    "    # Incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = ['respondent_id', 'date_flight_local'] + shap_columns + [f'base_prob{class_suffix}', out_prob_col] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def predict_and_explain(model_prom, model_det, df, features_dummy):\n",
    "    \"\"\"\n",
    "    Realiza predicciones y genera explicaciones para modelos de promotores y detractores\n",
    "    para todo el dataframe.\n",
    "\n",
    "    Args:\n",
    "    - model_prom: Modelo entrenado para predecir promotores.\n",
    "    - model_det: Modelo entrenado para predecir detractores.\n",
    "    - df: DataFrame con los datos.\n",
    "    - features_dummy: Lista de características utilizadas para las predicciones.\n",
    "\n",
    "    Returns:\n",
    "    - Df final con .data, .values, .base_value, y predicciones.\n",
    "    \"\"\"\n",
    "    # 1. Asumiendo que las funciones de cálculo de SHAP y probabilidad ya están implementadas y ajustadas para usar df\n",
    "    df_contrib = calculate_SHAP_and_probability_binary(model_prom, model_det, df)\n",
    "\n",
    "    # 3. Convertir valores SHAP a probabilidad\n",
    "    df_probability_prom = from_shap_to_probability_binary(df_contrib, features_dummy, 'promoter_binary')\n",
    "    df_probability_det = from_shap_to_probability_binary(df_contrib, features_dummy, 'detractor_binary')\n",
    "\n",
    "    # 4. Concatenar DataFrames para ambos modelos\n",
    "    df_probability_prom = df_probability_prom.reset_index(drop=True)\n",
    "    df_probability_det = df_probability_det.reset_index(drop=True)\n",
    "    unique_columns_det = [col for col in df_probability_det.columns if col not in df_probability_prom.columns]\n",
    "    df_probability_binary = pd.concat([df_probability_prom, df_probability_det[unique_columns_det]], axis=1)\n",
    "\n",
    "    # 5. Calcular columnas NPS con la diferencia entre _prom y _det\n",
    "    for column in df_probability_binary.columns:\n",
    "        if '_prom' in column:\n",
    "            base_name = column.split('_prom')[0]\n",
    "            det_column = f'{base_name}_det'\n",
    "            if det_column in df_probability_binary.columns:\n",
    "                nps_column = f'{base_name}_nps'\n",
    "                df_probability_binary[nps_column] = df_probability_binary[column] - df_probability_binary[det_column]\n",
    "\n",
    "    return df_probability_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50bd57b7-8b5d-4a65-9694-2f5bb8b92467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def calculate_SHAP_and_probability_binary(model_promoter, model_detractor, test_set):\n",
    "    # Predicciones para el modelo de promotores\n",
    "    promoter_test_set = test_set.drop(['promoter_binary'], axis=1, errors='ignore')\n",
    "    predictions_promoter = pd.DataFrame(model_promoter.predict(promoter_test_set), columns=[\"prediction_prom\"])\n",
    "    proba_promoter = pd.DataFrame(model_promoter.predict_proba(promoter_test_set))[[1]].rename(columns={1: \"out_prob_prom\"})\n",
    "    \n",
    "    # Predicciones para el modelo de detractores\n",
    "    detractor_test_set = test_set.drop(['detractor_binary'], axis=1, errors='ignore')\n",
    "    predictions_detractor = pd.DataFrame(model_detractor.predict(detractor_test_set), columns=[\"prediction_det\"])\n",
    "    proba_detractor = pd.DataFrame(model_detractor.predict_proba(detractor_test_set))[[1]].rename(columns={1: \"out_prob_det\"})\n",
    "    \n",
    "    # Combinar resultados de predicción\n",
    "    prediction = pd.concat([predictions_promoter, proba_promoter, predictions_detractor, proba_detractor, test_set.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo promotor\n",
    "    shap_Explainer_promoter = shap.TreeExplainer(model_promoter)\n",
    "    shap_values_promoter = shap_Explainer_promoter.shap_values(promoter_test_set)\n",
    "    feature_names = [i for i in promoter_test_set.columns]\n",
    "    shap_values_prom = pd.DataFrame(shap_values_promoter, columns=[f\"{i}_prom\" for i in feature_names])\n",
    "    shap_values_prom[\"base_value_prom\"] = shap_Explainer_promoter.expected_value\n",
    "    shap_values_prom[\"out_value_prom\"] = shap_values_prom.sum(axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo detractor\n",
    "    shap_Explainer_detractor = shap.TreeExplainer(model_detractor)\n",
    "    shap_values_detractor = shap_Explainer_detractor.shap_values(detractor_test_set)\n",
    "    shap_values_det = pd.DataFrame(shap_values_detractor, columns=[f\"{i}_det\" for i in feature_names])\n",
    "    shap_values_det[\"base_value_det\"] = shap_Explainer_detractor.expected_value\n",
    "    shap_values_det[\"out_value_det\"] = shap_values_det.sum(axis=1)\n",
    "    \n",
    "    # Combinar SHAP values con predicciones\n",
    "    output_df = pd.concat([prediction, shap_values_prom, shap_values_det], axis=1)\n",
    "    \n",
    "    # Devolver el dataframe de salida y los explicadores SHAP\n",
    "    return output_df, shap_Explainer_promoter, shap_Explainer_detractor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [col for col in df.columns if col.endswith(class_suffix)]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    \n",
    "    # Convertir el valor base a probabilidades y actualizar el nombre de la columna\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "    \n",
    "    # Convertir valores SHAP a probabilidades sin cambiar los nombres de las columnas\n",
    "    for col in shap_columns:\n",
    "        output_df[col] = inv_logit(output_df[col])\n",
    "    \n",
    "    # Asegurarse de incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = shap_columns + [f'base_prob{class_suffix}'] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def adjust_shap_values_binary(shap_values, base_prob, out_prob):\n",
    "    \"\"\"Ajustar los valores SHAP para un modelo binario basado en la distancia.\"\"\"\n",
    "    # Calcular la distancia total deseada entre la probabilidad base y la de salida\n",
    "    total_distance = out_prob - base_prob\n",
    "    # Calcular la suma total de los valores SHAP\n",
    "    total_shap = np.sum(shap_values)\n",
    "    # Calcular el factor de ajuste si la suma total de SHAP no es cero\n",
    "    adjustment_factor = total_distance / total_shap if total_shap != 0 else 0\n",
    "    # Ajustar los valores SHAP\n",
    "    return shap_values * adjustment_factor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [f'{feature}{class_suffix}' for feature in features_dummy if f'{feature}{class_suffix}' in df.columns]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    out_prob_col = f'out_prob{class_suffix}'\n",
    "\n",
    "    # Calcular la probabilidad base usando softmax o inv_logit según sea apropiado\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "\n",
    "    for index, row in output_df.iterrows():\n",
    "        # Extraer los valores SHAP para ajustar\n",
    "        shap_values = row[shap_columns].values\n",
    "        # Calcular los valores SHAP ajustados\n",
    "        adjusted_shap_values = adjust_shap_values_binary(shap_values, row[f'base_prob{class_suffix}'], row[out_prob_col])\n",
    "        # Actualizar el DataFrame con los valores SHAP ajustados\n",
    "        output_df.loc[index, shap_columns] = adjusted_shap_values\n",
    "\n",
    "    # Incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = shap_columns + [f'base_prob{class_suffix}', out_prob_col] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def predict_and_explain(model_prom, model_det, df, features_dummy, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Realiza predicciones y genera explicaciones para modelos de promotores y detractores\n",
    "    dentro de un rango de fechas específico.\n",
    "\n",
    "    Args:\n",
    "    - model_prom: Modelo entrenado para predecir promotores.\n",
    "    - model_det: Modelo entrenado para predecir detractores.\n",
    "    - df: DataFrame con los datos.\n",
    "    - features_dummy: Lista de características utilizadas para las predicciones.\n",
    "    - start_date: Fecha de inicio para los datos de prueba (formato 'YYYY-MM-DD').\n",
    "    - end_date: Fecha de fin para los datos de prueba (formato 'YYYY-MM-DD').\n",
    "\n",
    "    Returns:\n",
    "    - Nada, pero guarda los objetos de explicación SHAP con nombres que reflejan el mes y año.\n",
    "    \"\"\"\n",
    "    # 1. Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df['date_flight_local'] >= start_date) & (df['date_flight_local'] < end_date)]\n",
    "\n",
    "    # 2. Asumiendo que las funciones de cálculo de SHAP y probabilidad ya están implementadas y ajustadas para usar df_filtered\n",
    "    df_contrib, explainer_prom, explainer_det = calculate_SHAP_and_probability_binary(model_prom, model_det, df_filtered[features_dummy])\n",
    "\n",
    "    # 3. Convertir valores SHAP a probabilidad\n",
    "    df_probability_prom = from_shap_to_probability_binary(df_contrib, features_dummy, 'promoter_binary')\n",
    "    df_probability_det = from_shap_to_probability_binary(df_contrib, features_dummy, 'detractor_binary')\n",
    "\n",
    "    # 4. Concatenar DataFrames para ambos modelos\n",
    "    df_probability_prom = df_probability_prom.reset_index(drop=True)\n",
    "    df_probability_det = df_probability_det.reset_index(drop=True)\n",
    "    unique_columns_det = [col for col in df_probability_det.columns if col not in df_probability_prom.columns]\n",
    "    df_probability_binary = pd.concat([df_probability_prom, df_probability_det[unique_columns_det]], axis=1)\n",
    "\n",
    "    # 5. Calcular columnas NPS con la diferencia entre _prom y _det\n",
    "    for column in df_probability_binary.columns:\n",
    "        if '_prom' in column:\n",
    "            base_name = column.split('_prom')[0]\n",
    "            det_column = f'{base_name}_det'\n",
    "            if det_column in df_probability_binary.columns:\n",
    "                nps_column = f'{base_name}_nps'\n",
    "                df_probability_binary[nps_column] = df_probability_binary[column] - df_probability_binary[det_column]\n",
    "\n",
    "    # 6. Agregar variables y valores SHAP para crear una explicación general\n",
    "    # num_vars = ['delay_departure','ticket_price']\n",
    "    num_vars = ['ticket_price', 'load_factor']\n",
    "    bin_vars = ['otp15_takeoff']\n",
    "    # bin_vars = ['otp15_takeoff'] + [col for col in df_nps_tkt.columns if 'country_agg' in col]\n",
    "    # cat_vars=['segment']\n",
    "    # bin_vars=[]\n",
    "    cat_vars=[]\n",
    "    touchpoints = [feat for feat in features_dummy if feat not in bin_vars + num_vars]\n",
    "    values_nps_sum = [df_probability_binary[f'{feat}_nps'].mean()*100 for feat in features_dummy]\n",
    "    num_var_scores = [df_probability_binary[num_var].mean() for num_var in num_vars]\n",
    "    bin_vars_scores = []\n",
    "    for var in bin_vars:\n",
    "        if var == 'otp15_takeoff':\n",
    "            # Para 'otp15_takeoff', calcula el porcentaje de 0s.\n",
    "            score = (df_probability_binary[df_probability_binary[var] == 0][var].count() / \n",
    "                     df_probability_binary[var].count()) * 100\n",
    "        else:\n",
    "            # Para cualquier otra variable, calcula el porcentaje de 1s.\n",
    "            score = (df_probability_binary[df_probability_binary[var] == 1][var].count() / \n",
    "                     df_probability_binary[var].count()) * 100\n",
    "        bin_vars_scores.append(score)\n",
    "    # Continuación después de calcular bin_vars_scores\n",
    "    cat_vars_scores = []\n",
    "    for cat_var in cat_vars:\n",
    "        cat_vars_scores.append(0)\n",
    "\n",
    "    satisfaction_scores = [df_probability_binary[df_probability_binary[tp] >= 8][tp].count() / df_probability_binary[tp].count() * 100 for tp in touchpoints]\n",
    "    shap_data = np.array(num_var_scores + bin_vars_scores + satisfaction_scores)\n",
    "    base_value_nps_sum = df_probability_binary['base_prob_prom'].mean() * 100 - df_probability_binary['base_prob_det'].mean() * 100\n",
    "    shap_values = np.array(values_nps_sum)  # Convertimos la lista en un array 2D\n",
    "    features_names = np.array(features_dummy)\n",
    "    explainer = shap.Explanation(values=shap_values, \n",
    "                                 base_values=base_value_nps_sum, \n",
    "                                 data=shap_data, \n",
    "                                 feature_names=features_names)\n",
    "        \n",
    "    return explainer, df_probability_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448b685-9945-45ab-80e2-f2823e19ffdc",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d714a8ac-b83b-4fae-9e4d-e4c6e584d4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['ticket_price', 'load_factor', 'otp15_takeoff', 'bkg_200_journey_preparation', 'pfl_100_checkin', \n",
    "                  'pfl_200_security', 'pfl_300_lounge', 'pfl_500_boarding', 'ifl_300_cabin', \n",
    "                  'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife', 'ifl_400_food_drink', \n",
    "                  'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', \n",
    "                  'loy_200_loyalty_programme', 'img_310_ease_contact_phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fead2328-72ea-48fc-8a8c-d9eaa1870d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names=['PROM','DET']\n",
    "clf_model={}\n",
    "for name in model_names:\n",
    "    path_model=f'pipeline_output/CatBoostClassifier_cv_{name}.pkl'\n",
    "    # Cargar el modelo desde el archivo .pkl\n",
    "    with open(path_model, 'rb') as file:\n",
    "        clf_model[name] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fe8e9c6-52f6-4a60-9909-f63135b0b346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data to predict\n",
    "df_predict = pd.read_csv(f\"pipeline_output/data_for_historic_prediction.csv\")\n",
    "    \n",
    "# Asegurarse de que 'date_flight_local' esté en formato datetime\n",
    "df_predict['date_flight_local'] = pd.to_datetime(df_predict['date_flight_local'])\n",
    "df_predict = df_predict[df_predict['date_flight_local'].dt.year == 2023]\n",
    "    \n",
    "df_predict = df_predict[df_predict['date_flight_local'].dt.month == 1]\n",
    "\n",
    "def filter_data_by_quarter(df, quarter):\n",
    "    # Definir los rangos de fechas para cada trimestre\n",
    "    quarters = {\n",
    "        \"q1\": (1, 3),\n",
    "        \"q2\": (4, 6),\n",
    "        \"q3\": (7, 9),\n",
    "        \"q4\": (10, 12)\n",
    "    }\n",
    "\n",
    "    # Obtener el rango de meses para el trimestre especificado\n",
    "    start_month, end_month = quarters[quarter]\n",
    "\n",
    "    # Filtrar el DataFrame por el rango de fechas del trimestre\n",
    "    df_filtered = df[df['date_flight_local'].dt.month.between(start_month, end_month)]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "quarters = ['q1']\n",
    "\n",
    "for quarter in quarters:\n",
    "    df_predict = filter_data_by_quarter(df_predict, quarter)\n",
    "    # Perform prediction and add the probabilities to the dataframe\n",
    "    test_set = df_predict.drop(['respondent_id'], axis=1, errors='ignore')\n",
    "    df_probabilities = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], df_predict, features)\n",
    "    # explainer, df_probabilities = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], test_set, features, '2023-01-01','2023-01-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51fbe0-25d3-498d-8489-82487a8b9967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d6f08ef-eec1-431a-8d4d-33beed048f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3086387843699623"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probabilities['out_prob_nps'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3153d712-6662-4f7e-bca2-3751fe26cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05299504376253408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probabilities['out_prob_nps'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74114f96-4c5e-42ac-b4b0-4d73a530358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Rename columns, add insert date and select columns to save\n",
    "# df_probabilities['insert_date_ci'] = STR_EXECUTION_DATE\n",
    "# df_probabilities['model_version']=f'{model_year}-{model_month}-{model_day}'\n",
    "# df_probabilities = df_probabilities[config['PREDICT']['COLUMNS_SAVE']]\n",
    "\n",
    "# Save the prediction results to S3\n",
    "df_probabilities.to_csv(save_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
