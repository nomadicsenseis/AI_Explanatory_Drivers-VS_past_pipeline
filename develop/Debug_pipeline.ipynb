{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fba4cd-3f9d-40bf-b9ae-6b031bf8b581",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.8/site-packages (5.18.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from plotly) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->plotly) (3.0.4)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: boto3==1.19.12 in /opt/conda/lib/python3.8/site-packages (1.19.12)\n",
      "Requirement already satisfied: botocore<1.23.0,>=1.22.12 in /opt/conda/lib/python3.8/site-packages (from boto3==1.19.12) (1.22.12)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3==1.19.12) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3==1.19.12) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.23.0,>=1.22.12->boto3==1.19.12) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.23.0,>=1.22.12->boto3==1.19.12) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.12->boto3==1.19.12) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.8/site-packages (from s3fs) (1.22.12)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from s3fs) (2021.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore>=1.12.91->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore>=1.12.91->s3fs) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm) (1.20.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm) (1.10.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: shap in /opt/conda/lib/python3.8/site-packages (0.44.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from shap) (1.20.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from shap) (1.3.4)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.8/site-packages (from shap) (4.62.3)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.8/site-packages (from shap) (21.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.8/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.8/site-packages (from shap) (0.54.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.8/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>20.9->shap) (3.0.4)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba->shap) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba->shap) (69.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->shap) (2021.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: catboost in /opt/conda/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.8/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from catboost) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from catboost) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.8/site-packages (from catboost) (1.3.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.8/site-packages (from catboost) (5.18.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24->catboost) (2021.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from plotly->catboost) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from plotly->catboost) (21.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install boto3==1.19.12\n",
    "!pip install s3fs\n",
    "!pip install lightgbm\n",
    "!pip install shap\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af85a46-bdbe-42b9-acb1-d3003c753d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from datetime import datetime, timedelta\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import os\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "import datetime\n",
    "import boto3\n",
    "import s3fs\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Models\n",
    "from catboost import CatBoostClassifier, cv, Pool\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "# Random\n",
    "import random\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c1a007-6c0d-4daa-bf30-bf616db01f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('pipeline_output/incremental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f82984f-d7e7-4d83-a61c-9620efae666a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NaN\n",
       "1        NaN\n",
       "2        NaN\n",
       "3        NaN\n",
       "4      0.810\n",
       "        ... \n",
       "7543     NaN\n",
       "7544     NaN\n",
       "7545     NaN\n",
       "7546     NaN\n",
       "7547     NaN\n",
       "Name: load_factor, Length: 7548, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['load_factor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69292ce9-c093-46e6-a72b-42831c010179",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a55f7a-29b1-4abe-a32a-343668467098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def calculate_SHAP_and_probability_binary(model_promoter, model_detractor, df):\n",
    "    # Extraer ID y fechas, manteniendo el índice\n",
    "    id_df = df[['respondent_id', 'date_flight_local']]\n",
    "    \n",
    "    # Preparar el conjunto de datos para predicciones, excluyendo ID y fechas\n",
    "    test_set = df.drop(['respondent_id', 'date_flight_local'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Predicciones y probabilidades para promotores\n",
    "    promoter_test_set = test_set.drop(['promoter_binary'], axis=1, errors='ignore')\n",
    "    predictions_promoter = pd.DataFrame(model_promoter.predict(promoter_test_set), index=promoter_test_set.index, columns=[\"prediction_prom\"])\n",
    "    proba_promoter = pd.DataFrame(model_promoter.predict_proba(promoter_test_set)[:, 1], index=promoter_test_set.index, columns=[\"out_prob_prom\"])\n",
    "    \n",
    "    # Predicciones y probabilidades para detractores\n",
    "    detractor_test_set = test_set.drop(['detractor_binary'], axis=1, errors='ignore')\n",
    "    predictions_detractor = pd.DataFrame(model_detractor.predict(detractor_test_set), index=detractor_test_set.index, columns=[\"prediction_det\"])\n",
    "    proba_detractor = pd.DataFrame(model_detractor.predict_proba(detractor_test_set)[:, 1], index=detractor_test_set.index, columns=[\"out_prob_det\"])\n",
    "    \n",
    "    # Combinar resultados de predicción, manteniendo el índice original\n",
    "    prediction = pd.concat([id_df, test_set, predictions_promoter, proba_promoter, predictions_detractor, proba_detractor], axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo promotor\n",
    "    shap_Explainer_promoter = shap.TreeExplainer(model_promoter)\n",
    "    shap_values_promoter = shap_Explainer_promoter.shap_values(promoter_test_set)\n",
    "    feature_names = [i for i in promoter_test_set.columns]\n",
    "    shap_values_prom = pd.DataFrame(shap_values_promoter, index=promoter_test_set.index, columns=[f\"{i}_prom\" for i in feature_names])\n",
    "    shap_values_prom[\"base_value_prom\"] = shap_Explainer_promoter.expected_value\n",
    "    shap_values_prom[\"out_value_prom\"] = shap_values_prom.sum(axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo detractor\n",
    "    shap_Explainer_detractor = shap.TreeExplainer(model_detractor)\n",
    "    shap_values_detractor = shap_Explainer_detractor.shap_values(detractor_test_set)\n",
    "    shap_values_det = pd.DataFrame(shap_values_detractor, index=detractor_test_set.index, columns=[f\"{i}_det\" for i in feature_names])\n",
    "    shap_values_det[\"base_value_det\"] = shap_Explainer_detractor.expected_value\n",
    "    shap_values_det[\"out_value_det\"] = shap_values_det.sum(axis=1)\n",
    "    \n",
    "    # Combinar SHAP values con predicciones, manteniendo el índice original\n",
    "    output_df = pd.concat([prediction, shap_values_prom, shap_values_det], axis=1)\n",
    "    \n",
    "    # Devolver el dataframe de salida\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [col for col in df.columns if col.endswith(class_suffix)]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    \n",
    "    # Convertir el valor base a probabilidades y actualizar el nombre de la columna\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "    \n",
    "    # Convertir valores SHAP a probabilidades sin cambiar los nombres de las columnas\n",
    "    for col in shap_columns:\n",
    "        output_df[col] = inv_logit(output_df[col])\n",
    "    \n",
    "    # Asegurarse de incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = ['respondent_id', 'date_flight_local'] + shap_columns + [f'base_prob{class_suffix}'] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    return output_df\n",
    "\n",
    "def adjust_shap_values_binary(shap_values, base_prob, out_prob):\n",
    "    \"\"\"Ajustar los valores SHAP para un modelo binario basado en la distancia.\"\"\"\n",
    "    # Calcular la distancia total deseada entre la probabilidad base y la de salida\n",
    "    total_distance = out_prob - base_prob\n",
    "    # Calcular la suma total de los valores SHAP\n",
    "    total_shap = np.sum(shap_values)\n",
    "    # Calcular el factor de ajuste si la suma total de SHAP no es cero\n",
    "    adjustment_factor = total_distance / total_shap if total_shap != 0 else 0\n",
    "    # Ajustar los valores SHAP\n",
    "    return shap_values * adjustment_factor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [f'{feature}{class_suffix}' for feature in features_dummy if f'{feature}{class_suffix}' in df.columns]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    out_prob_col = f'out_prob{class_suffix}'\n",
    "\n",
    "    # Calcular la probabilidad base usando softmax o inv_logit según sea apropiado\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "\n",
    "    for index, row in output_df.iterrows():\n",
    "        # Extraer los valores SHAP para ajustar\n",
    "        shap_values = row[shap_columns].values\n",
    "        # Calcular los valores SHAP ajustados\n",
    "        adjusted_shap_values = adjust_shap_values_binary(shap_values, row[f'base_prob{class_suffix}'], row[out_prob_col])\n",
    "        # Actualizar el DataFrame con los valores SHAP ajustados\n",
    "        output_df.loc[index, shap_columns] = adjusted_shap_values\n",
    "\n",
    "    # Incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = ['respondent_id', 'date_flight_local'] + shap_columns + [f'base_prob{class_suffix}', out_prob_col] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def predict_and_explain(model_prom, model_det, df, features_dummy):\n",
    "    \"\"\"\n",
    "    Realiza predicciones y genera explicaciones para modelos de promotores y detractores\n",
    "    para todo el dataframe.\n",
    "\n",
    "    Args:\n",
    "    - model_prom: Modelo entrenado para predecir promotores.\n",
    "    - model_det: Modelo entrenado para predecir detractores.\n",
    "    - df: DataFrame con los datos.\n",
    "    - features_dummy: Lista de características utilizadas para las predicciones.\n",
    "\n",
    "    Returns:\n",
    "    - Df final con .data, .values, .base_value, y predicciones.\n",
    "    \"\"\"\n",
    "    # 1. Asumiendo que las funciones de cálculo de SHAP y probabilidad ya están implementadas y ajustadas para usar df\n",
    "    df_contrib = calculate_SHAP_and_probability_binary(model_prom, model_det, df)\n",
    "\n",
    "    # 3. Convertir valores SHAP a probabilidad\n",
    "    df_probability_prom = from_shap_to_probability_binary(df_contrib, features_dummy, 'promoter_binary')\n",
    "    df_probability_det = from_shap_to_probability_binary(df_contrib, features_dummy, 'detractor_binary')\n",
    "\n",
    "    # 4. Concatenar DataFrames para ambos modelos\n",
    "    df_probability_prom = df_probability_prom.reset_index(drop=True)\n",
    "    df_probability_det = df_probability_det.reset_index(drop=True)\n",
    "    unique_columns_det = [col for col in df_probability_det.columns if col not in df_probability_prom.columns]\n",
    "    df_probability_binary = pd.concat([df_probability_prom, df_probability_det[unique_columns_det]], axis=1)\n",
    "\n",
    "    # 5. Calcular columnas NPS con la diferencia entre _prom y _det\n",
    "    for column in df_probability_binary.columns:\n",
    "        if '_prom' in column:\n",
    "            base_name = column.split('_prom')[0]\n",
    "            det_column = f'{base_name}_det'\n",
    "            if det_column in df_probability_binary.columns:\n",
    "                nps_column = f'{base_name}_nps'\n",
    "                df_probability_binary[nps_column] = df_probability_binary[column] - df_probability_binary[det_column]\n",
    "\n",
    "    return df_probability_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bd57b7-8b5d-4a65-9694-2f5bb8b92467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def calculate_SHAP_and_probability_binary(model_promoter, model_detractor, test_set):\n",
    "    # Predicciones para el modelo de promotores\n",
    "    promoter_test_set = test_set.drop(['promoter_binary'], axis=1, errors='ignore')\n",
    "    predictions_promoter = pd.DataFrame(model_promoter.predict(promoter_test_set), columns=[\"prediction_prom\"])\n",
    "    proba_promoter = pd.DataFrame(model_promoter.predict_proba(promoter_test_set))[[1]].rename(columns={1: \"out_prob_prom\"})\n",
    "    \n",
    "    # Predicciones para el modelo de detractores\n",
    "    detractor_test_set = test_set.drop(['detractor_binary'], axis=1, errors='ignore')\n",
    "    predictions_detractor = pd.DataFrame(model_detractor.predict(detractor_test_set), columns=[\"prediction_det\"])\n",
    "    proba_detractor = pd.DataFrame(model_detractor.predict_proba(detractor_test_set))[[1]].rename(columns={1: \"out_prob_det\"})\n",
    "    \n",
    "    # Combinar resultados de predicción\n",
    "    prediction = pd.concat([predictions_promoter, proba_promoter, predictions_detractor, proba_detractor, test_set.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo promotor\n",
    "    shap_Explainer_promoter = shap.TreeExplainer(model_promoter)\n",
    "    shap_values_promoter = shap_Explainer_promoter.shap_values(promoter_test_set)\n",
    "    feature_names = [i for i in promoter_test_set.columns]\n",
    "    shap_values_prom = pd.DataFrame(shap_values_promoter, columns=[f\"{i}_prom\" for i in feature_names])\n",
    "    shap_values_prom[\"base_value_prom\"] = shap_Explainer_promoter.expected_value\n",
    "    shap_values_prom[\"out_value_prom\"] = shap_values_prom.sum(axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo detractor\n",
    "    shap_Explainer_detractor = shap.TreeExplainer(model_detractor)\n",
    "    shap_values_detractor = shap_Explainer_detractor.shap_values(detractor_test_set)\n",
    "    shap_values_det = pd.DataFrame(shap_values_detractor, columns=[f\"{i}_det\" for i in feature_names])\n",
    "    shap_values_det[\"base_value_det\"] = shap_Explainer_detractor.expected_value\n",
    "    shap_values_det[\"out_value_det\"] = shap_values_det.sum(axis=1)\n",
    "    \n",
    "    # Combinar SHAP values con predicciones\n",
    "    output_df = pd.concat([prediction, shap_values_prom, shap_values_det], axis=1)\n",
    "    \n",
    "    # Devolver el dataframe de salida y los explicadores SHAP\n",
    "    return output_df, shap_Explainer_promoter, shap_Explainer_detractor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [col for col in df.columns if col.endswith(class_suffix)]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    \n",
    "    # Convertir el valor base a probabilidades y actualizar el nombre de la columna\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "    \n",
    "    # Convertir valores SHAP a probabilidades sin cambiar los nombres de las columnas\n",
    "    for col in shap_columns:\n",
    "        output_df[col] = inv_logit(output_df[col])\n",
    "    \n",
    "    # Asegurarse de incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = shap_columns + [f'base_prob{class_suffix}'] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def adjust_shap_values_binary(shap_values, base_prob, out_prob):\n",
    "    \"\"\"Ajustar los valores SHAP para un modelo binario basado en la distancia.\"\"\"\n",
    "    # Calcular la distancia total deseada entre la probabilidad base y la de salida\n",
    "    total_distance = out_prob - base_prob\n",
    "    # Calcular la suma total de los valores SHAP\n",
    "    total_shap = np.sum(shap_values)\n",
    "    # Calcular el factor de ajuste si la suma total de SHAP no es cero\n",
    "    adjustment_factor = total_distance / total_shap if total_shap != 0 else 0\n",
    "    # Ajustar los valores SHAP\n",
    "    return shap_values * adjustment_factor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [f'{feature}{class_suffix}' for feature in features_dummy if f'{feature}{class_suffix}' in df.columns]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    out_prob_col = f'out_prob{class_suffix}'\n",
    "\n",
    "    # Calcular la probabilidad base usando softmax o inv_logit según sea apropiado\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "\n",
    "    for index, row in output_df.iterrows():\n",
    "        # Extraer los valores SHAP para ajustar\n",
    "        shap_values = row[shap_columns].values\n",
    "        # Calcular los valores SHAP ajustados\n",
    "        adjusted_shap_values = adjust_shap_values_binary(shap_values, row[f'base_prob{class_suffix}'], row[out_prob_col])\n",
    "        # Actualizar el DataFrame con los valores SHAP ajustados\n",
    "        output_df.loc[index, shap_columns] = adjusted_shap_values\n",
    "\n",
    "    # Incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = shap_columns + [f'base_prob{class_suffix}', out_prob_col] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def predict_and_explain(model_prom, model_det, df, features_dummy, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Realiza predicciones y genera explicaciones para modelos de promotores y detractores\n",
    "    dentro de un rango de fechas específico.\n",
    "\n",
    "    Args:\n",
    "    - model_prom: Modelo entrenado para predecir promotores.\n",
    "    - model_det: Modelo entrenado para predecir detractores.\n",
    "    - df: DataFrame con los datos.\n",
    "    - features_dummy: Lista de características utilizadas para las predicciones.\n",
    "    - start_date: Fecha de inicio para los datos de prueba (formato 'YYYY-MM-DD').\n",
    "    - end_date: Fecha de fin para los datos de prueba (formato 'YYYY-MM-DD').\n",
    "\n",
    "    Returns:\n",
    "    - Nada, pero guarda los objetos de explicación SHAP con nombres que reflejan el mes y año.\n",
    "    \"\"\"\n",
    "    # 1. Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df['date_flight_local'] >= start_date) & (df['date_flight_local'] < end_date)]\n",
    "\n",
    "    # 2. Asumiendo que las funciones de cálculo de SHAP y probabilidad ya están implementadas y ajustadas para usar df_filtered\n",
    "    df_contrib, explainer_prom, explainer_det = calculate_SHAP_and_probability_binary(model_prom, model_det, df_filtered[features_dummy])\n",
    "\n",
    "    # 3. Convertir valores SHAP a probabilidad\n",
    "    df_probability_prom = from_shap_to_probability_binary(df_contrib, features_dummy, 'promoter_binary')\n",
    "    df_probability_det = from_shap_to_probability_binary(df_contrib, features_dummy, 'detractor_binary')\n",
    "\n",
    "    # 4. Concatenar DataFrames para ambos modelos\n",
    "    df_probability_prom = df_probability_prom.reset_index(drop=True)\n",
    "    df_probability_det = df_probability_det.reset_index(drop=True)\n",
    "    unique_columns_det = [col for col in df_probability_det.columns if col not in df_probability_prom.columns]\n",
    "    df_probability_binary = pd.concat([df_probability_prom, df_probability_det[unique_columns_det]], axis=1)\n",
    "\n",
    "    # 5. Calcular columnas NPS con la diferencia entre _prom y _det\n",
    "    for column in df_probability_binary.columns:\n",
    "        if '_prom' in column:\n",
    "            base_name = column.split('_prom')[0]\n",
    "            det_column = f'{base_name}_det'\n",
    "            if det_column in df_probability_binary.columns:\n",
    "                nps_column = f'{base_name}_nps'\n",
    "                df_probability_binary[nps_column] = df_probability_binary[column] - df_probability_binary[det_column]\n",
    "\n",
    "    # 6. Agregar variables y valores SHAP para crear una explicación general\n",
    "    # num_vars = ['delay_departure','ticket_price']\n",
    "    num_vars = ['ticket_price', 'load_factor']\n",
    "    bin_vars = ['otp15_takeoff']\n",
    "    # bin_vars = ['otp15_takeoff'] + [col for col in df_nps_tkt.columns if 'country_agg' in col]\n",
    "    # cat_vars=['segment']\n",
    "    # bin_vars=[]\n",
    "    cat_vars=[]\n",
    "    touchpoints = [feat for feat in features_dummy if feat not in bin_vars + num_vars]\n",
    "    values_nps_sum = [df_probability_binary[f'{feat}_nps'].mean()*100 for feat in features_dummy]\n",
    "    num_var_scores = [df_probability_binary[num_var].mean() for num_var in num_vars]\n",
    "    bin_vars_scores = []\n",
    "    for var in bin_vars:\n",
    "        if var == 'otp15_takeoff':\n",
    "            # Para 'otp15_takeoff', calcula el porcentaje de 0s.\n",
    "            score = (df_probability_binary[df_probability_binary[var] == 0][var].count() / \n",
    "                     df_probability_binary[var].count()) * 100\n",
    "        else:\n",
    "            # Para cualquier otra variable, calcula el porcentaje de 1s.\n",
    "            score = (df_probability_binary[df_probability_binary[var] == 1][var].count() / \n",
    "                     df_probability_binary[var].count()) * 100\n",
    "        bin_vars_scores.append(score)\n",
    "    # Continuación después de calcular bin_vars_scores\n",
    "    cat_vars_scores = []\n",
    "    for cat_var in cat_vars:\n",
    "        cat_vars_scores.append(0)\n",
    "\n",
    "    satisfaction_scores = [df_probability_binary[df_probability_binary[tp] >= 8][tp].count() / df_probability_binary[tp].count() * 100 for tp in touchpoints]\n",
    "    shap_data = np.array(num_var_scores + bin_vars_scores + satisfaction_scores)\n",
    "    base_value_nps_sum = df_probability_binary['base_prob_prom'].mean() * 100 - df_probability_binary['base_prob_det'].mean() * 100\n",
    "    shap_values = np.array(values_nps_sum)  # Convertimos la lista en un array 2D\n",
    "    features_names = np.array(features_dummy)\n",
    "    explainer = shap.Explanation(values=shap_values, \n",
    "                                 base_values=base_value_nps_sum, \n",
    "                                 data=shap_data, \n",
    "                                 feature_names=features_names)\n",
    "        \n",
    "    return explainer, df_probability_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448b685-9945-45ab-80e2-f2823e19ffdc",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d714a8ac-b83b-4fae-9e4d-e4c6e584d4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['ticket_price', 'load_factor', 'otp15_takeoff', 'bkg_200_journey_preparation', 'pfl_100_checkin', \n",
    "                  'pfl_200_security', 'pfl_300_lounge', 'pfl_500_boarding', 'ifl_300_cabin', \n",
    "                  'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife', 'ifl_400_food_drink', \n",
    "                  'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', \n",
    "                  'loy_200_loyalty_programme', 'img_310_ease_contact_phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fead2328-72ea-48fc-8a8c-d9eaa1870d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names=['PROM','DET']\n",
    "clf_model={}\n",
    "for name in model_names:\n",
    "    path_model=f'pipeline_output/CatBoostClassifier_cv_{name}.pkl'\n",
    "    # Cargar el modelo desde el archivo .pkl\n",
    "    with open(path_model, 'rb') as file:\n",
    "        clf_model[name] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe8e9c6-52f6-4a60-9909-f63135b0b346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data to predict\n",
    "df_predict = pd.read_csv(f\"pipeline_output/data_for_historic_prediction.csv\")\n",
    "    \n",
    "# Asegurarse de que 'date_flight_local' esté en formato datetime\n",
    "df_predict['date_flight_local'] = pd.to_datetime(df_predict['date_flight_local'])\n",
    "df_predict = df_predict[df_predict['date_flight_local'].dt.year == 2023]\n",
    "    \n",
    "df_predict = df_predict[df_predict['date_flight_local'].dt.month == 1]\n",
    "\n",
    "def filter_data_by_quarter(df, quarter):\n",
    "    # Definir los rangos de fechas para cada trimestre\n",
    "    quarters = {\n",
    "        \"q1\": (1, 3),\n",
    "        \"q2\": (4, 6),\n",
    "        \"q3\": (7, 9),\n",
    "        \"q4\": (10, 12)\n",
    "    }\n",
    "\n",
    "    # Obtener el rango de meses para el trimestre especificado\n",
    "    start_month, end_month = quarters[quarter]\n",
    "\n",
    "    # Filtrar el DataFrame por el rango de fechas del trimestre\n",
    "    df_filtered = df[df['date_flight_local'].dt.month.between(start_month, end_month)]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "quarters = ['q1']\n",
    "\n",
    "for quarter in quarters:\n",
    "    df_predict = filter_data_by_quarter(df_predict, quarter)\n",
    "    # Perform prediction and add the probabilities to the dataframe\n",
    "    test_set = df_predict.drop(['respondent_id'], axis=1, errors='ignore')\n",
    "    # df_probabilities = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], df_predict, features)\n",
    "    explainer, df_probabilities = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], test_set, features, '2023-01-01','2023-01-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51fbe0-25d3-498d-8489-82487a8b9967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6f08ef-eec1-431a-8d4d-33beed048f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30753317961568943"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probabilities['out_prob_nps'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3153d712-6662-4f7e-bca2-3751fe26cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30753317961568943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probabilities['out_prob_nps'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74114f96-4c5e-42ac-b4b0-4d73a530358d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # Rename columns, add insert date and select columns to save\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# df_probabilities['insert_date_ci'] = STR_EXECUTION_DATE\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df_probabilities['model_version']=f'{model_year}-{model_month}-{model_day}'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df_probabilities = df_probabilities[config['PREDICT']['COLUMNS_SAVE']]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save the prediction results to S3\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_probabilities\u001b[38;5;241m.\u001b[39mto_csv(\u001b[43msave_path\u001b[49m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Rename columns, add insert date and select columns to save\n",
    "# df_probabilities['insert_date_ci'] = STR_EXECUTION_DATE\n",
    "# df_probabilities['model_version']=f'{model_year}-{model_month}-{model_day}'\n",
    "# df_probabilities = df_probabilities[config['PREDICT']['COLUMNS_SAVE']]\n",
    "\n",
    "# Save the prediction results to S3\n",
    "df_probabilities.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b05ce3-4341-4bf9-8801-17a767a811a9",
   "metadata": {},
   "source": [
    "# CHECK historic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf24eb5-23b4-411f-a346-f5907f72fe88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hist=pd.read_csv('pipeline_output/historic_predictions (8).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ddb12-b92c-4cc4-9533-f4baeb7e3bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hist['date_flight_local']=pd.to_datetime(df_hist['date_flight_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab763f4-08f3-49d4-8cd3-7f5d284336e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_shaps(df,features_dummy, start_date, end_date):\n",
    "\n",
    "    # 1. Filtrar el DataFrame por el rango de fechas\n",
    "    df_probability_binary = df[(df['date_flight_local'] >= start_date) & (df['date_flight_local'] < end_date)]\n",
    "    \n",
    "    # df_probability_binary.drop(columns=['respondent_id','date_flight_local','model_version','insert_date_ci'])\n",
    "\n",
    "    # 2. Asumiendo que las funciones de cálculo de SHAP y probabilidad ya están implementadas y ajustadas para usar df_filtered\n",
    "    # 6. Agregar variables y valores SHAP para crear una explicación general\n",
    "    # num_vars = ['delay_departure','ticket_price']\n",
    "    num_vars = ['ticket_price', 'load_factor']\n",
    "    bin_vars = ['otp15_takeoff']\n",
    "    # bin_vars = ['otp15_takeoff'] + [col for col in df_nps_tkt.columns if 'country_agg' in col]\n",
    "    # cat_vars=['segment']\n",
    "    # bin_vars=[]\n",
    "    cat_vars=[]\n",
    "    touchpoints = [feat for feat in features_dummy if feat not in bin_vars + num_vars]\n",
    "    \n",
    "    # values_nps_sum = [pd.to_numeric(df_probability_binary[f'{feat}_nps'], errors='coerce').mean()*100 for feat in features_dummy]\n",
    "    values_nps_sum = [df_probability_binary[f'{feat}_nps'].mean()*100 for feat in features_dummy]\n",
    "    num_var_scores = [df_probability_binary[num_var].mean() for num_var in num_vars]\n",
    "    bin_vars_scores = []\n",
    "    for var in bin_vars:\n",
    "        if var == 'otp15_takeoff':\n",
    "            # Para 'otp15_takeoff', calcula el porcentaje de 0s.\n",
    "            score = (df_probability_binary[df_probability_binary[var] == 0][var].count() / \n",
    "                     df_probability_binary[var].count()) * 100\n",
    "        else:\n",
    "            # Para cualquier otra variable, calcula el porcentaje de 1s.\n",
    "            score = (df_probability_binary[df_probability_binary[var] == 1][var].count() / \n",
    "                     df_probability_binary[var].count()) * 100\n",
    "        bin_vars_scores.append(score)\n",
    "    # Continuación después de calcular bin_vars_scores\n",
    "    cat_vars_scores = []\n",
    "    for cat_var in cat_vars:\n",
    "        cat_vars_scores.append(0)\n",
    "\n",
    "    satisfaction_scores = [df_probability_binary[df_probability_binary[tp] >= 8][tp].count() / df_probability_binary[tp].count() * 100 for tp in touchpoints]\n",
    "    shap_data = np.array(num_var_scores + bin_vars_scores + satisfaction_scores)\n",
    "    base_value_nps_sum = df_probability_binary['base_prob_nps'].mean() * 100\n",
    "    shap_values = np.array(values_nps_sum)  # Convertimos la lista en un array 2D\n",
    "    features_names = np.array(features_dummy)\n",
    "    explainer = shap.Explanation(values=shap_values, \n",
    "                                 base_values=base_value_nps_sum, \n",
    "                                 data=shap_data, \n",
    "                                 feature_names=features_names)\n",
    "    pred_nps = df_probability_binary['out_prob_nps'].mean()\n",
    "        \n",
    "    return explainer, df_probability_binary, pred_nps\n",
    "\n",
    "# model_prom=train_results['models']['promoter_binary']\n",
    "# model_det=train_results['models']['detractor_binary']\n",
    "\n",
    "# Lista de años de interés\n",
    "years_of_interest = [2019, 2022, 2023, 2024]\n",
    "explanations=[]\n",
    "features_dummy = ['ticket_price', 'load_factor', 'otp15_takeoff', 'bkg_200_journey_preparation', 'pfl_100_checkin', \n",
    "                  'pfl_200_security', 'pfl_300_lounge', 'pfl_500_boarding', 'ifl_300_cabin', \n",
    "                  'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife', 'ifl_400_food_drink', \n",
    "                  'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', \n",
    "                  'loy_200_loyalty_programme', 'img_310_ease_contact_phone']\n",
    "for year in years_of_interest:\n",
    "    for month in range(1, 13):\n",
    "        if year == 2024 and month > 4:\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "        start_date = f\"{year}-{month:02d}-01\"\n",
    "        end_date = (pd.to_datetime(start_date) + MonthEnd(1)).strftime('%Y-%m-%d')\n",
    "\n",
    "#         # Filtrar el DataFrame por las fechas de inicio y fin\n",
    "#         df_filtered = df_nps_tkt[(df_nps_tkt['date_flight_local'] >= start_date) & (df_nps_tkt['date_flight_local'] < end_date)]\n",
    "\n",
    "#         # Verificar si df_filtered está vacío\n",
    "#         if df_filtered.empty:\n",
    "#             print(f\"No hay datos para el rango de fechas desde {start_date} hasta {end_date}.\")\n",
    "#             continue\n",
    "\n",
    "        explanation, df_probability_binary, pred_nps = aggregate_shaps(df_hist,features_dummy, start_date, end_date)\n",
    "    \n",
    "       \n",
    "    \n",
    "        # No romper el bucle; solo continuar si 'explanation' es None\n",
    "        if explanation is not None:\n",
    "            print(explanation)\n",
    "            explanations.append(explanation)\n",
    "            # Guardar cada explicación con un nombre de archivo que refleje el mes y año\n",
    "            file_name = f'pipeline_output/raw_explanations/explanation_{month}_{year}.pkl'\n",
    "            with open(file_name, 'wb') as file:\n",
    "                pickle.dump(explanation, file)\n",
    "            print(file_name)\n",
    "            print(pred_nps)\n",
    "            shap.plots.waterfall(explanation, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ae1b3-5207-4429-9df8-c63731db211a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_uplifting_explanation(explanation2, explanation1):\n",
    "    \"\"\"\n",
    "    Create a new Explanation object representing the uplifting between two Explanation objects.\n",
    "\n",
    "    Parameters:\n",
    "        - explanation1: The first shap.Explanation object.\n",
    "        - explanation2: The second shap.Explanation object.\n",
    "\n",
    "    Returns:\n",
    "        - A new shap.Explanation object representing the uplifting.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in values, base_values, and data\n",
    "    diff_values = explanation2.values - explanation1.values\n",
    "    \n",
    "    diff_base_values = explanation1.base_values + sum(explanation1.values)\n",
    "    print(sum(explanation1.values))\n",
    "    diff_data = explanation2.data - explanation1.data\n",
    "\n",
    "    # Create a new Explanation object with the difference values\n",
    "    diff_explanation = shap.Explanation(values=diff_values, base_values=diff_base_values, data=diff_data,\n",
    "                                        feature_names=explanation1.feature_names)\n",
    "\n",
    "    return diff_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86786b-7656-408d-b562-dd8744d80bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_explanation(year, month):\n",
    "    \"\"\"Cargar un objeto de explicación desde un archivo.\"\"\"\n",
    "    file_name = f'pipeline_output/raw_explanations/explanation_{month}_{year}.pkl'\n",
    "    try:\n",
    "        with open(file_name, 'rb') as file:\n",
    "            explanation = pickle.load(file)\n",
    "        return explanation\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No explanation file found for {month}/{year}.\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso: Comparar febrero de 2023 con febrero de 2022\n",
    "explanation_2024_01 = load_explanation(2023, 2)\n",
    "explanation_2024_02 = load_explanation(2024, 1)\n",
    "\n",
    "if explanation_2024_01 is not None and explanation_2024_02 is not None:\n",
    "    print('2024: Januery vs Februery')\n",
    "    uplifting_explanation = create_uplifting_explanation(explanation_2024_02, explanation_2024_01)\n",
    "    # Procesar o visualizar el uplifting_explanation según sea necesario\n",
    "    shap.plots.waterfall(uplifting_explanation, max_display=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8f270-3017-44ab-adb7-509710e7d224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_monthly_explanations(start_year, start_month, end_year, end_month):\n",
    "    \"\"\"\n",
    "    Compara automáticamente cada mes con el mismo mes del año anterior o con 2019 si el año es 2022,\n",
    "    desde una fecha de inicio dada, y devuelve un diccionario con los objetos de explicación.\n",
    "\n",
    "    Args:\n",
    "    - start_year: Año de inicio para las comparaciones.\n",
    "    - start_month: Mes de inicio para las comparaciones.\n",
    "    - end_year: Año final para las comparaciones.\n",
    "    - end_month: Mes final para las comparaciones.\n",
    "\n",
    "    Returns:\n",
    "    - Un diccionario con las comparaciones de objetos de explicación. Las claves son las fechas de comparación.\n",
    "    \"\"\"\n",
    "    uplifting_explanations_dict = {}\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            if year == start_year and month < start_month:\n",
    "                continue\n",
    "            if year == end_year and month > end_month:\n",
    "                break\n",
    "\n",
    "            # Ajuste para el año 2022: comparar con 2019\n",
    "            if year == 2022:\n",
    "                previous_year_explanation = load_explanation(2019, month)\n",
    "            else:\n",
    "                previous_year_explanation = load_explanation(year - 1, month)\n",
    "\n",
    "            current_explanation = load_explanation(year, month)\n",
    "\n",
    "            if current_explanation is not None and previous_year_explanation is not None:\n",
    "                uplifting_explanation = create_uplifting_explanation(previous_year_explanation,current_explanation)\n",
    "                # Ajuste en la clave para reflejar la comparación especial del año 2022 con 2019\n",
    "                if year == 2022:\n",
    "                    date_key = f\"2019-{month:02d} to {year}-{month:02d}\"\n",
    "                else:\n",
    "                    date_key = f\"{year-1}-{month:02d} to {year}-{month:02d}\"\n",
    "                uplifting_explanations_dict[date_key] = uplifting_explanation\n",
    "            print(date_key)  \n",
    "            shap.plots.waterfall(uplifting_explanations_dict[date_key], max_display=20)\n",
    "\n",
    "    return uplifting_explanations_dict\n",
    "\n",
    "# Ejemplo de uso\n",
    "uplifting_explanations_dict = compare_monthly_explanations(2023, 3, 2024, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62872cf-c334-43d5-aed3-37db8b3f8210",
   "metadata": {},
   "source": [
    "# Check incremental"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec5bedac-a325-431e-a651-4c0aad88cb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "features= ['respondent_id', 'date_flight_local', 'cabin_in_surveyed_flight', 'haul', 'insert_date_ci', 'bkg_200_journey_preparation', 'pfl_100_checkin', \n",
    "                  'pfl_200_security', 'pfl_300_lounge', 'pfl_500_boarding', 'ifl_300_cabin', \n",
    "                  'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife', 'ifl_400_food_drink', \n",
    "                  'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', \n",
    "                  'loy_200_loyalty_programme', 'img_310_ease_contact_phone']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e0be125-9d1f-4093-8289-ceb9fc09082d",
   "metadata": {
    "tags": []
   },
   "source": [
    "import dask.dataframe as dd\n",
    "insert_date_ci='2024-04-12'\n",
    "dask_dd = dd.read_csv(f's3://iberia-data-lake/customer/nps_surveys/export_historic/insert_date_ci={insert_date_ci}/*',dtype={'arr_100_arrivals': 'float64',\n",
    "       'ifl_300_cabin': 'float64',\n",
    "       'pfl_100_checkin': 'float64',\n",
    "       'pfl_500_boarding': 'float64'},low_memory=False)[features].compute()\n",
    "dask_dd.insert_date_ci.value_counts().sort_index().tail(30)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a99bf88f-9531-4a2c-92ad-3cc066cc1d71",
   "metadata": {
    "tags": []
   },
   "source": [
    "dask_dd[dask_dd.insert_date_ci=='2024-04-12'].sort_values(by=['respondent_id'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55016fdb-a195-4201-84fd-cfabba254461",
   "metadata": {
    "tags": []
   },
   "source": [
    "import dask.dataframe as dd\n",
    "insert_date_ci='2024-04-11'\n",
    "dask_dd2 = dd.read_csv(f's3://iberia-data-lake/customer/nps_surveys/export_historic/insert_date_ci={insert_date_ci}/*',dtype={'arr_100_arrivals': 'float64',\n",
    "       'ifl_300_cabin': 'float64',\n",
    "       'pfl_100_checkin': 'float64',\n",
    "       'pfl_500_boarding': 'float64'},low_memory=False)[features].compute()\n",
    "dask_dd2.insert_date_ci.value_counts().sort_index().tail(30)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "444ed6f5-7988-4ebc-9928-8e78d14b4963",
   "metadata": {
    "tags": []
   },
   "source": [
    "dask_dd2[dask_dd2.insert_date_ci=='2024-04-11'].sort_values(by=['respondent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8abd77d9-b4ac-488a-a339-27cd64f7b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "S3_BUCKET_NPS = 'iberia-data-lake'\n",
    "insert_date_ci='2024-04-12'\n",
    "today_nps_surveys_prefix = f'customer/nps_surveys/export_historic/insert_date_ci={insert_date_ci}/'\n",
    "dir_dict = 's3://iberia-data-lake/customer/nps_surveys/nps_dictionaries'\n",
    "\n",
    "lf_dir = 's3://ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8df83da-b3ad-4215-8d6b-5a4a1f274ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from datetime import datetime, timedelta\n",
    "    # Convert to datetime object\n",
    "    execution_date = datetime.strptime(insert_date_ci, \"%Y-%m-%d\")\n",
    "\n",
    "    # Calculate yesterday's date\n",
    "    yesterday_date = execution_date - timedelta(days=1)\n",
    "    # Format dates as strings for S3 prefixes\n",
    "    today_date_str = execution_date.strftime(\"%Y-%m-%d\")\n",
    "    yesterday_date_str = yesterday_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    yesterday_nps_surveys_prefix= f'customer/nps_surveys/export_historic/insert_date_ci={yesterday_date_str}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74d7dab1-f945-4baa-93dd-b8882b7a4638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # READ TODAY DATA (HISTORIC NPS)\n",
    "    s3_keys = [item.key for item in s3_resource.Bucket(S3_BUCKET_NPS).objects.filter(Prefix=today_nps_surveys_prefix)]\n",
    "    preprocess_paths = [f\"s3://{S3_BUCKET_NPS}/{key}\" for key in s3_keys]\n",
    "\n",
    "    df_nps_historic = pd.DataFrame()\n",
    "    for file in preprocess_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        df_nps_historic = pd.concat([df_nps_historic, df], axis=0)\n",
    "    df_nps_historic = df_nps_historic.reset_index(drop=True)\n",
    "\n",
    "    # READ PREVIOUS NPS DATA (FOR INCREMENTAL)\n",
    "    yesterday_s3_keys = [item.key for item in s3_resource.Bucket(S3_BUCKET_NPS).objects.filter(Prefix=yesterday_nps_surveys_prefix)]\n",
    "    yesterday_preprocess_paths = [f\"s3://{S3_BUCKET_NPS}/{key}\" for key in yesterday_s3_keys]\n",
    "\n",
    "    df_nps_yesterday = pd.DataFrame()\n",
    "    for file in yesterday_preprocess_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        df_nps_yesterday = pd.concat([df_nps_yesterday, df], axis=0)\n",
    "    df_nps_yesterday = df_nps_yesterday.reset_index(drop=True)\n",
    "\n",
    "    # INCREMENTAL NPS  \n",
    "    df_nps_incremental = pd.merge(df_nps_historic, df_nps_yesterday, how='left', indicator=True, on=df_nps_historic.columns.tolist())\n",
    "    df_nps_incremental = df_nps_incremental[df_nps_incremental['_merge'] == 'left_only']\n",
    "    df_nps_incremental = df_nps_incremental.drop(columns=['_merge'])\n",
    "    df_nps_incremental = df_nps_incremental.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4116bbb-31fb-4714-bc52-3349f4ed3239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-02000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-03000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-04000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-05000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-06000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-07000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-08000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-09000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-10000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-11000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-12000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-13000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-14000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-15000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-16000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-17000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-18000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-19000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-20000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-21000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-22000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-23000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-24000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-25000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-26000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-27000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-28000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-29000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-30000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-03-31000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-01000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-02000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-03000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-04000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-05000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-06000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-07000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-08000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-09000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-10000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-11000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-12000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-13000', 'ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-14000']\n",
      "Caught EmptyDataError for file: ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/nps_operation_data_2024-04-12000, skipping...\n"
     ]
    }
   ],
   "source": [
    "# READ LF DATA SOURCE\n",
    "    # lf_dir = 's3://ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/'    \n",
    "load_factor_prefix = 's3://ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/' \n",
    "\n",
    "    # Assume rol for prod\n",
    "sts_client = boto3.client('sts')\n",
    "assumed_role = sts_client.assume_role(\n",
    "    RoleArn=\"arn:aws:iam::320714865578:role/ibdata-prod-role-assume-customer-services-from-ibdata-aip-prod\",\n",
    "    RoleSessionName=\"test\"\n",
    ")\n",
    "credentials = assumed_role['Credentials']\n",
    "fs = s3fs.S3FileSystem(key=credentials['AccessKeyId'], secret=credentials['SecretAccessKey'], token=credentials['SessionToken'])\n",
    "\n",
    "    # Listall the files\n",
    "load_factor_list = fs.ls(load_factor_prefix)\n",
    "print(load_factor_list)\n",
    "    \n",
    "dataframes = []\n",
    "for file_path in load_factor_list:\n",
    "    try:\n",
    "        file_info = fs.info(file_path)\n",
    "        if file_info['Size'] == 0:\n",
    "            continue\n",
    "\n",
    "        with fs.open(f's3://{file_path}') as f:\n",
    "            if today_date_str in file_path:\n",
    "                df_lf_incremental = pd.read_csv(f)\n",
    "            df = pd.read_csv(f)\n",
    "            dataframes.append(df)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Caught EmptyDataError for file: {file_path}, skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "if dataframes:\n",
    "    df_lf_historic = pd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    df_lf_historic = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d7f9a75-08e0-4f27-9b84-b019c4ebb526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userlog: ETL 1.0 Filter dataframes by carrier code.\n",
      "userlog: ETL 2.0 Transform date column to datetime format.\n",
      "userlog: ETL 3.0 Filter out covid years.\n",
      "userlog: ETL 4.0 Create otp, promoter, detractor and load factor columns.\n"
     ]
    }
   ],
   "source": [
    "    # 1. Filter dataframes by carrier code.\n",
    "    print(\"userlog: ETL 1.0 Filter dataframes by carrier code.\")\n",
    "    # NPS HISTORIC\n",
    "    condition_1 = (df_nps_historic['operating_airline_code'].isin(['IB', 'YW']))\n",
    "    condition_2 = ((df_nps_historic['invitegroup_ib'] != 3) | (df_nps_historic['invitegroup_ib'].isnull()))\n",
    "    condition_3 = (df_nps_historic['invitegroup'] == 2)\n",
    "    \n",
    "    df_nps_historic = df_nps_historic.loc[condition_1 & (condition_2 & condition_3)]\n",
    "\n",
    "    # NPS INCREMENTAL\n",
    "    condition_1 = (df_nps_incremental['operating_airline_code'].isin(['IB', 'YW']))\n",
    "    condition_2 = ((df_nps_incremental['invitegroup_ib'] != 3) | (df_nps_incremental['invitegroup_ib'].isnull()))\n",
    "    condition_3 = (df_nps_incremental['invitegroup'] == 2)\n",
    "\n",
    "    df_nps_incremental = df_nps_incremental.loc[condition_1 & (condition_2 & condition_3)]\n",
    "\n",
    "    # LOAD FACTOR HISTORIC\n",
    "    df_lf_historic = df_lf_historic.loc[(df_lf_historic['operating_carrier'].isin(['IB', 'YW']))]\n",
    "\n",
    "    # LOAD FACTOR INCREMENTAL\n",
    "    df_lf_incremental = df_lf_incremental.loc[(df_lf_incremental['operating_carrier'].isin(['IB', 'YW']))]\n",
    "\n",
    "\n",
    "    # 2. Transform date column to datetime format\n",
    "    print(\"userlog: ETL 2.0 Transform date column to datetime format.\")\n",
    "    delay_features = ['real_departure_time_local', 'scheduled_departure_time_local']\n",
    "    for feat in delay_features:\n",
    "        df_nps_historic[feat] = pd.to_datetime(df_nps_historic[feat], format=\"%Y%m%d %H:%M:%S\", errors = 'coerce')\n",
    "        df_nps_incremental[feat] = pd.to_datetime(df_nps_incremental[feat], format=\"%Y%m%d %H:%M:%S\", errors = 'coerce')\n",
    "            \n",
    "    df_nps_historic['delay_departure'] = (df_nps_historic['real_departure_time_local'] - df_nps_historic['scheduled_departure_time_local']).dt.total_seconds()/60\n",
    "    df_nps_incremental['delay_departure'] = (df_nps_incremental['real_departure_time_local'] - df_nps_incremental['scheduled_departure_time_local']).dt.total_seconds()/60\n",
    "    \n",
    "    # NPS\n",
    "    df_nps_historic['date_flight_local'] = pd.to_datetime(df_nps_historic['date_flight_local'])\n",
    "    df_nps_incremental['date_flight_local'] = pd.to_datetime(df_nps_incremental['date_flight_local'])\n",
    "\n",
    "    # Load Factor\n",
    "    df_lf_historic['flight_date_local'] = pd.to_datetime(df_lf_historic['flight_date_local'])\n",
    "    df_lf_incremental['flight_date_local'] = pd.to_datetime(df_lf_incremental['flight_date_local'])\n",
    "\n",
    "    # 3. Filter out covid years\n",
    "    print(\"userlog: ETL 3.0 Filter out covid years.\")\n",
    "    # NPS (historic)\n",
    "    df_nps_historic = df_nps_historic[df_nps_historic['date_flight_local'].dt.year >= 2019]\n",
    "    df_nps_historic = df_nps_historic[~df_nps_historic['date_flight_local'].dt.year.isin([2020, 2021])]\n",
    "    # Load factor (historic)\n",
    "    df_lf_historic = df_lf_historic[df_lf_historic['flight_date_local'].dt.year >= 2019]\n",
    "    df_lf_historic = df_lf_historic[~df_lf_historic['flight_date_local'].dt.year.isin([2020, 2021])]\n",
    "\n",
    "    # 4. Create otp, promoter, detractor and load factor columns.\n",
    "    print(\"userlog: ETL 4.0 Create otp, promoter, detractor and load factor columns.\")\n",
    "    # OTP\n",
    "    df_nps_historic['otp15_takeoff'] = (df_nps_historic['delay_departure'] > 15).astype(int)\n",
    "    df_nps_incremental['otp15_takeoff'] = (df_nps_incremental['delay_departure'] > 15).astype(int)\n",
    "\n",
    "    # Promoter and Detractor columns\n",
    "    df_nps_historic[\"promoter_binary\"] = df_nps_historic[\"nps_category\"].apply(lambda x: 1 if x == \"Promoter\" else 0)\n",
    "    df_nps_historic[\"detractor_binary\"] = df_nps_historic[\"nps_category\"].apply(lambda x: 1 if x == \"Detractor\" else 0)\n",
    "    df_nps_incremental[\"promoter_binary\"] = df_nps_incremental[\"nps_category\"].apply(lambda x: 1 if x == \"Promoter\" else 0)\n",
    "    df_nps_incremental[\"detractor_binary\"] = df_nps_incremental[\"nps_category\"].apply(lambda x: 1 if x == \"Detractor\" else 0)\n",
    "\n",
    "    # Load Factor\n",
    "    df_lf_historic['load_factor_business'] = df_lf_historic['pax_business'] / df_lf_historic['capacity_business']\n",
    "    df_lf_historic['load_factor_premium_ec'] = df_lf_historic['pax_premium_ec'] / df_lf_historic['capacity_premium_ec']\n",
    "    df_lf_historic['load_factor_economy'] = df_lf_historic['pax_economy'] / df_lf_historic['capacity_economy']\n",
    "\n",
    "    df_lf_incremental['load_factor_business'] = df_lf_incremental['pax_business'] / df_lf_incremental['capacity_business']\n",
    "    df_lf_incremental['load_factor_premium_ec'] = df_lf_incremental['pax_premium_ec'] / df_lf_incremental['capacity_premium_ec']\n",
    "    df_lf_incremental['load_factor_economy'] = df_lf_incremental['pax_economy'] / df_lf_incremental['capacity_economy']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "29330acb-e5ae-4ecc-9ffd-0086cfd40472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userlog: ETL 5.0 Merge dataframes.\n",
      "date_flight_local             datetime64[ns]\n",
      "flight_date_utc                       object\n",
      "operating_airline_code                object\n",
      "surveyed_flight_number                 int64\n",
      "boardpoint_stn_code_actual            object\n",
      "offpoint_stn_code_actual              object\n",
      "haul                                  object\n",
      "calc_dep_diff                          int64\n",
      "punctuality                           object\n",
      "capacity_business                      int64\n",
      "pax_business                           int64\n",
      "capacity_premium_ec                    int64\n",
      "pax_premium_ec                         int64\n",
      "capacity_economy                       int64\n",
      "pax_economy                            int64\n",
      "load_factor_business                 float64\n",
      "load_factor_premium_ec               float64\n",
      "load_factor_economy                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "    # 5. Merge dataframes.\n",
    "    print(\"userlog: ETL 5.0 Merge dataframes.\")\n",
    "    cabin_to_load_factor_column = {\n",
    "        'Economy': 'load_factor_economy',\n",
    "        'Business': 'load_factor_business',\n",
    "        'Premium Economy': 'load_factor_premium_ec'\n",
    "    }\n",
    "\n",
    "    # HISTORIC\n",
    "    df_lf_historic.columns = ['date_flight_local' if x=='flight_date_local' else \n",
    "                                    'operating_airline_code' if x=='operating_carrier' else\n",
    "                                    'surveyed_flight_number' if x=='op_flight_num' else\n",
    "                                    x for x in df_lf_historic.columns]\n",
    "    \n",
    "    print(df_lf_historic.dtypes)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a6c7cd8-11a5-4758-a718-6c6ec9c4e8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respondent_id                              int64\n",
      "sample_id                                  int64\n",
      "surveyed_flight_number                   float64\n",
      "date_flight_local                 datetime64[ns]\n",
      "scheduled_departure_time_local    datetime64[ns]\n",
      "                                       ...      \n",
      "codeshare                                 object\n",
      "delay_departure                          float64\n",
      "otp15_takeoff                              int64\n",
      "promoter_binary                            int64\n",
      "detractor_binary                           int64\n",
      "Length: 580, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_nps_historic.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45ee3cde-09d3-40eb-988d-3bfe0c241e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5cfd6b70-1678-496f-becc-a68a517890f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_flight_local             datetime64[ns]\n",
      "flight_date_utc                       object\n",
      "operating_airline_code                object\n",
      "surveyed_flight_number               float64\n",
      "boardpoint_stn_code_actual            object\n",
      "offpoint_stn_code_actual              object\n",
      "haul                                  object\n",
      "calc_dep_diff                          int64\n",
      "punctuality                           object\n",
      "capacity_business                      int64\n",
      "pax_business                           int64\n",
      "capacity_premium_ec                    int64\n",
      "pax_premium_ec                         int64\n",
      "capacity_economy                       int64\n",
      "pax_economy                            int64\n",
      "cabin_in_surveyed_flight              object\n",
      "load_factor                          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_long.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6de98875-cb8c-4de3-a91c-a6d9556f3f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df_lf_historic.copy()\n",
    "\n",
    "# List of columns to transform\n",
    "load_factor_columns = ['load_factor_business', 'load_factor_premium_ec', 'load_factor_economy']\n",
    "\n",
    "# Automatically determine id_vars by excluding load_factor_columns from all columns\n",
    "id_vars = [col for col in df.columns if col not in load_factor_columns]\n",
    "\n",
    "# Reshaping the DataFrame while dynamically keeping all other columns\n",
    "df = pd.melt(df, id_vars=id_vars, \n",
    "                  value_vars=load_factor_columns,\n",
    "                  var_name='cabin_in_surveyed_flight', value_name='load_factor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9104d539-d3df-4197-be01-0bf5564e48ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_flight_local</th>\n",
       "      <th>flight_date_utc</th>\n",
       "      <th>operating_airline_code</th>\n",
       "      <th>surveyed_flight_number</th>\n",
       "      <th>boardpoint_stn_code_actual</th>\n",
       "      <th>offpoint_stn_code_actual</th>\n",
       "      <th>haul</th>\n",
       "      <th>calc_dep_diff</th>\n",
       "      <th>punctuality</th>\n",
       "      <th>capacity_business</th>\n",
       "      <th>pax_business</th>\n",
       "      <th>capacity_premium_ec</th>\n",
       "      <th>pax_premium_ec</th>\n",
       "      <th>capacity_economy</th>\n",
       "      <th>pax_economy</th>\n",
       "      <th>cabin_in_surveyed_flight</th>\n",
       "      <th>load_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>IB</td>\n",
       "      <td>3237</td>\n",
       "      <td>FCO</td>\n",
       "      <td>MAD</td>\n",
       "      <td>SH</td>\n",
       "      <td>72</td>\n",
       "      <td>OTP15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>192</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-15</td>\n",
       "      <td>2023-10-15</td>\n",
       "      <td>IB</td>\n",
       "      <td>6301</td>\n",
       "      <td>MAD</td>\n",
       "      <td>SJU</td>\n",
       "      <td>LH</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>266</td>\n",
       "      <td>Business</td>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>IB</td>\n",
       "      <td>3148</td>\n",
       "      <td>MAD</td>\n",
       "      <td>PRG</td>\n",
       "      <td>SH</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>150</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>IB</td>\n",
       "      <td>6461</td>\n",
       "      <td>MAD</td>\n",
       "      <td>GYE</td>\n",
       "      <td>LH</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>268</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>IB</td>\n",
       "      <td>3149</td>\n",
       "      <td>PRG</td>\n",
       "      <td>MAD</td>\n",
       "      <td>SH</td>\n",
       "      <td>-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548517</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>IB</td>\n",
       "      <td>6342</td>\n",
       "      <td>SAL</td>\n",
       "      <td>MAD</td>\n",
       "      <td>LH</td>\n",
       "      <td>-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>238</td>\n",
       "      <td>Economy</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548518</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>IB</td>\n",
       "      <td>6118</td>\n",
       "      <td>MIA</td>\n",
       "      <td>MAD</td>\n",
       "      <td>LH</td>\n",
       "      <td>46</td>\n",
       "      <td>OTP15</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>242</td>\n",
       "      <td>239</td>\n",
       "      <td>Economy</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548519</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>IB</td>\n",
       "      <td>6832</td>\n",
       "      <td>SCL</td>\n",
       "      <td>MAD</td>\n",
       "      <td>LH</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>293</td>\n",
       "      <td>292</td>\n",
       "      <td>Economy</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548520</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>IB</td>\n",
       "      <td>6588</td>\n",
       "      <td>BOG</td>\n",
       "      <td>MAD</td>\n",
       "      <td>LH</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>293</td>\n",
       "      <td>283</td>\n",
       "      <td>Economy</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548521</th>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>IB</td>\n",
       "      <td>3431</td>\n",
       "      <td>CDG</td>\n",
       "      <td>MAD</td>\n",
       "      <td>SH</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>133</td>\n",
       "      <td>Economy</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1548522 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_flight_local flight_date_utc operating_airline_code  \\\n",
       "0              2023-09-20      2023-09-20                     IB   \n",
       "1              2023-10-15      2023-10-15                     IB   \n",
       "2              2023-10-16      2023-10-16                     IB   \n",
       "3              2023-10-16      2023-10-16                     IB   \n",
       "4              2023-10-16      2023-10-16                     IB   \n",
       "...                   ...             ...                    ...   \n",
       "1548517        2024-04-13      2024-04-14                     IB   \n",
       "1548518        2024-04-13      2024-04-14                     IB   \n",
       "1548519        2024-04-13      2024-04-14                     IB   \n",
       "1548520        2024-04-13      2024-04-14                     IB   \n",
       "1548521        2024-04-14      2024-04-14                     IB   \n",
       "\n",
       "         surveyed_flight_number boardpoint_stn_code_actual  \\\n",
       "0                          3237                        FCO   \n",
       "1                          6301                        MAD   \n",
       "2                          3148                        MAD   \n",
       "3                          6461                        MAD   \n",
       "4                          3149                        PRG   \n",
       "...                         ...                        ...   \n",
       "1548517                    6342                        SAL   \n",
       "1548518                    6118                        MIA   \n",
       "1548519                    6832                        SCL   \n",
       "1548520                    6588                        BOG   \n",
       "1548521                    3431                        CDG   \n",
       "\n",
       "        offpoint_stn_code_actual haul  calc_dep_diff punctuality  \\\n",
       "0                            MAD   SH             72       OTP15   \n",
       "1                            SJU   LH              3         NaN   \n",
       "2                            PRG   SH             -2         NaN   \n",
       "3                            GYE   LH             -1         NaN   \n",
       "4                            MAD   SH            -12         NaN   \n",
       "...                          ...  ...            ...         ...   \n",
       "1548517                      MAD   LH            -11         NaN   \n",
       "1548518                      MAD   LH             46       OTP15   \n",
       "1548519                      MAD   LH              0         NaN   \n",
       "1548520                      MAD   LH              9         NaN   \n",
       "1548521                      MAD   SH              2         NaN   \n",
       "\n",
       "         capacity_business  pax_business  capacity_premium_ec  pax_premium_ec  \\\n",
       "0                       12            12                    0               0   \n",
       "1                       19            18                    0               0   \n",
       "2                       16            16                    0               0   \n",
       "3                       19            19                    0               0   \n",
       "4                       12            12                    0               0   \n",
       "...                    ...           ...                  ...             ...   \n",
       "1548517                 19            19                    0               0   \n",
       "1548518                 29            29                   21              21   \n",
       "1548519                 31            31                   24              24   \n",
       "1548520                 31            31                   24              24   \n",
       "1548521                  8             7                    0               0   \n",
       "\n",
       "         capacity_economy  pax_economy cabin_in_surveyed_flight  load_factor  \n",
       "0                     199          192                 Business        1.000  \n",
       "1                     269          266                 Business        0.947  \n",
       "2                     156          150                 Business        1.000  \n",
       "3                     269          268                 Business        1.000  \n",
       "4                     162          162                 Business        1.000  \n",
       "...                   ...          ...                      ...          ...  \n",
       "1548517               269          238                  Economy        0.885  \n",
       "1548518               242          239                  Economy        0.988  \n",
       "1548519               293          292                  Economy        0.997  \n",
       "1548520               293          283                  Economy        0.966  \n",
       "1548521               168          133                  Economy        0.792  \n",
       "\n",
       "[1548522 rows x 17 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing the column names in 'cabin_in_surveyed_flight' with the desired cabin types\n",
    "df['cabin_in_surveyed_flight'] = df['cabin_in_surveyed_flight'].map({\n",
    "    'load_factor_business': 'Business',\n",
    "    'load_factor_premium_ec': 'Premium Economy',\n",
    "    'load_factor_economy': 'Economy'\n",
    "})\n",
    "\n",
    "df['operating_airline_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8a1a8812-04bb-4b0b-b23d-57f3ff9a11e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IB', 'YW'], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['operating_airline_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "265e0c7b-4534-46f7-b9b6-5f4635f99147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_flight_local</th>\n",
       "      <th>operating_airline_code</th>\n",
       "      <th>surveyed_flight_number</th>\n",
       "      <th>cabin_in_surveyed_flight</th>\n",
       "      <th>haul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-29</td>\n",
       "      <td>IB</td>\n",
       "      <td>3444.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-29</td>\n",
       "      <td>IB</td>\n",
       "      <td>577.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>IB</td>\n",
       "      <td>6253.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-03</td>\n",
       "      <td>IB</td>\n",
       "      <td>3398.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>IB</td>\n",
       "      <td>6132.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>2024-04-02</td>\n",
       "      <td>IB</td>\n",
       "      <td>6588.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7531</th>\n",
       "      <td>2024-03-29</td>\n",
       "      <td>YW</td>\n",
       "      <td>8966.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>YW</td>\n",
       "      <td>8391.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>2024-04-07</td>\n",
       "      <td>IB</td>\n",
       "      <td>6149.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>IB</td>\n",
       "      <td>3177.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7535 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_flight_local operating_airline_code  surveyed_flight_number  \\\n",
       "0           2024-03-29                     IB                3444.000   \n",
       "1           2024-03-29                     IB                 577.000   \n",
       "2           2024-04-04                     IB                6253.000   \n",
       "3           2024-04-03                     IB                3398.000   \n",
       "4           2024-04-04                     IB                6132.000   \n",
       "...                ...                    ...                     ...   \n",
       "7530        2024-04-02                     IB                6588.000   \n",
       "7531        2024-03-29                     YW                8966.000   \n",
       "7532        2024-04-05                     YW                8391.000   \n",
       "7533        2024-04-07                     IB                6149.000   \n",
       "7534        2024-04-05                     IB                3177.000   \n",
       "\n",
       "     cabin_in_surveyed_flight haul  \n",
       "0                     Economy   SH  \n",
       "1                     Economy   SH  \n",
       "2                     Economy   LH  \n",
       "3                     Economy   SH  \n",
       "4                     Economy   LH  \n",
       "...                       ...  ...  \n",
       "7530                  Economy   LH  \n",
       "7531                  Economy   SH  \n",
       "7532                  Economy   SH  \n",
       "7533                  Economy   LH  \n",
       "7534                  Economy   SH  \n",
       "\n",
       "[7535 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nps_incremental[['date_flight_local', 'operating_airline_code', 'surveyed_flight_number', 'cabin_in_surveyed_flight', 'haul']]\n",
    "df_long[['date_flight_local', 'operating_airline_code', 'surveyed_flight_number', 'cabin_in_surveyed_flight', 'haul']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68d55bde-634b-42a9-9a41-90fe63b630fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_flight_local</th>\n",
       "      <th>operating_airline_code</th>\n",
       "      <th>surveyed_flight_number</th>\n",
       "      <th>cabin_in_surveyed_flight</th>\n",
       "      <th>haul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>IB</td>\n",
       "      <td>3237.000</td>\n",
       "      <td>Business</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-15</td>\n",
       "      <td>IB</td>\n",
       "      <td>6301.000</td>\n",
       "      <td>Business</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>IB</td>\n",
       "      <td>3148.000</td>\n",
       "      <td>Business</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>IB</td>\n",
       "      <td>6461.000</td>\n",
       "      <td>Business</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>IB</td>\n",
       "      <td>3149.000</td>\n",
       "      <td>Business</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548517</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>IB</td>\n",
       "      <td>6342.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548518</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>IB</td>\n",
       "      <td>6118.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548519</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>IB</td>\n",
       "      <td>6832.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548520</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>IB</td>\n",
       "      <td>6588.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>LH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548521</th>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>IB</td>\n",
       "      <td>3431.000</td>\n",
       "      <td>Economy</td>\n",
       "      <td>SH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1548522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_flight_local operating_airline_code  surveyed_flight_number  \\\n",
       "0              2023-09-20                     IB                3237.000   \n",
       "1              2023-10-15                     IB                6301.000   \n",
       "2              2023-10-16                     IB                3148.000   \n",
       "3              2023-10-16                     IB                6461.000   \n",
       "4              2023-10-16                     IB                3149.000   \n",
       "...                   ...                    ...                     ...   \n",
       "1548517        2024-04-13                     IB                6342.000   \n",
       "1548518        2024-04-13                     IB                6118.000   \n",
       "1548519        2024-04-13                     IB                6832.000   \n",
       "1548520        2024-04-13                     IB                6588.000   \n",
       "1548521        2024-04-14                     IB                3431.000   \n",
       "\n",
       "        cabin_in_surveyed_flight haul  \n",
       "0                       Business   SH  \n",
       "1                       Business   LH  \n",
       "2                       Business   SH  \n",
       "3                       Business   LH  \n",
       "4                       Business   SH  \n",
       "...                          ...  ...  \n",
       "1548517                  Economy   LH  \n",
       "1548518                  Economy   LH  \n",
       "1548519                  Economy   LH  \n",
       "1548520                  Economy   LH  \n",
       "1548521                  Economy   SH  \n",
       "\n",
       "[1548522 rows x 5 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long[['date_flight_local', 'operating_airline_code', 'surveyed_flight_number', 'cabin_in_surveyed_flight', 'haul']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d0bb225f-c33e-4910-a512-af9463454fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intersection_df = pd.merge(\n",
    "    df_nps_incremental,\n",
    "    df,\n",
    "    how='left',\n",
    "    on=['date_flight_local', 'operating_airline_code', 'surveyed_flight_number', 'cabin_in_surveyed_flight', 'haul']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0fbf11e-5383-405f-9e8f-3815661be847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_df['load_factor'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9d42c6a-a36b-48ef-9294-2419872f4a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_incremental['haul'] = df_nps_incremental['haul'].replace('MH', 'SH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2ef9f-7a2c-41ac-8b0a-16261be8cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lf_historic['date_flight_local']=pd.to_datetime(df_lf_historic['date_flight_local'])\n",
    "df_lf_historic['surveyed_flight_number'] = df_lf_historic['surveyed_flight_number'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9833d761-9fde-437b-8010-f865b8b548b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_long['date_flight_local']=pd.to_datetime(df_long['date_flight_local'])\n",
    "df_long['surveyed_flight_number'] = df_long['surveyed_flight_number'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c61032b-6a94-48cc-9c6f-73d76d4e8024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    df_historic = pd.merge(df_nps_historic, df_long, \n",
    "                        how='left', \n",
    "                        on=['date_flight_local', 'operating_airline_code', 'surveyed_flight_number', 'cabin_in_surveyed_flight', 'haul'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e811e55b-4322-4aab-bd1d-ec8fb1b0f620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308777"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_historic['load_factor'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9801e28-4914-4717-a1ce-1b5609617f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 6. Filter out final columns for the model\n",
    "    print(\"userlog: ETL 6.0 Filter out final columns for the model\")\n",
    "    features_dummy = ['ticket_price', 'load_factor', 'otp15_takeoff'] + ['bkg_200_journey_preparation', 'pfl_100_checkin', \n",
    "                  'pfl_200_security', 'pfl_300_lounge', 'pfl_500_boarding', 'ifl_300_cabin', \n",
    "                  'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife', 'ifl_400_food_drink', \n",
    "                  'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', \n",
    "                  'loy_200_loyalty_programme', 'img_310_ease_contact_phone']\n",
    "\n",
    "    labels = ['promoter_binary', 'detractor_binary']\n",
    "\n",
    "    df_historic = df_historic[['respondent_id' , 'date_flight_local'] + features_dummy + labels]\n",
    "    df_incremental = df_incremental[['respondent_id' , 'date_flight_local'] + features_dummy + labels]\n",
    "\n",
    "    df_historic = df_historic.drop_duplicates()\n",
    "    df_incremental = df_incremental.drop_duplicates()\n",
    "    \n",
    "    print(\"userlog: Size of resulting df_historic:\", df_historic.shape)\n",
    "    print(\"userlog: Size of resulting df_incremental:\", df_incremental.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d957f-d0d7-455d-bfce-cd6479708527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
